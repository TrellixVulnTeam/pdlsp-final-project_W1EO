{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "985aef79-1fc6-40f1-8ae7-2f1bf9153441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config '/home/jupyter/vilio/py-bottom-up-attention/configs/VG-Detection/faster_rcnn_R_101_C4_attr_caffemaxpool.yaml' has no VERSION. Assuming it to be compatible with latest v2.\n",
      "Modifications for VG in ResNet Backbone (modeling/backbone/resnet.py):\n",
      "\tUsing pad 0 in stem max_pool instead of pad 1.\n",
      "\n",
      "Modifications for VG in RPN (modeling/proposal_generator/rpn.py):\n",
      "\tUse hidden dim 512 instead fo the same dim as Res4 (1024).\n",
      "\n",
      "Modifications for VG in RoI heads (modeling/roi_heads/roi_heads.py):\n",
      "\t1. Change the stride of conv1 and shortcut in Res5.Block1 from 2 to 1.\n",
      "\t2. Modifying all conv2 with (padding: 1 --> 2) and (dilation: 1 --> 2).\n",
      "\tFor more details, please check 'https://github.com/peteanderson80/bottom-up-attention/blob/master/models/vg/ResNet-101/faster_rcnn_end2end_final/test.prototxt'.\n",
      "\n",
      "Modifications for VG in RoI heads (modeling/roi_heads/fast_rcnn.py))\n",
      "\tEmbedding: 1601 --> 256\tLinear: 2304 --> 512\tLinear: 512 --> 401\n",
      "\n",
      "100%|███████████████████████████████████| 10001/10001 [1:04:51<00:00,  2.57it/s]\n"
     ]
    }
   ],
   "source": [
    "#Feature Extraction on the image dataset with 36 boxes for each image\n",
    "!cd ../vilio/py-bottom-up-attention; python detectron2_mscoco_proposal_maxnms.py --batchsize 5 --split img --weight vgattr \\\n",
    "--minboxes 36 --maxboxes 36\n",
    "# !cd ./vilio/py*/data; ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa93055-1a05-40b5-9951-0f71903ea53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install environment for UNITER\n",
    "!cd ./vilio/py-bottom-up-attention; pip install -r requirements.txt\n",
    "!pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
    "!cd ./vilio/py-bottom-up-attention; python setup.py build develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dca2745-3ef4-40f8-8bd9-c72d54ff0baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try on test dataset\n",
    "!cp -r ./input/facebook-hateful-meme-dataset/data/* ./vilio/py*/data/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11813a09-1a6b-4d36-ad59-c73cb3fbf582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/fvcore.git\n",
      "  Cloning https://github.com/facebookresearch/fvcore.git to /tmp/pip-req-build-3oc62b8r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/fvcore.git /tmp/pip-req-build-3oc62b8r\n",
      "  Resolved https://github.com/facebookresearch/fvcore.git to commit 8645461ff00ba0f7aab10248949c61061c905f70\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fvcore==0.1.5) (1.19.5)\n",
      "Requirement already satisfied: yacs>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from fvcore==0.1.5) (0.1.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from fvcore==0.1.5) (6.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from fvcore==0.1.5) (4.63.0)\n",
      "Requirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.7/site-packages (from fvcore==0.1.5) (1.1.0)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from fvcore==0.1.5) (9.0.1)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from fvcore==0.1.5) (0.8.9)\n",
      "Collecting iopath>=0.1.7\n",
      "  Using cached iopath-0.1.9-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from iopath>=0.1.7->fvcore==0.1.5) (2.4.0)\n",
      "Building wheels for collected packages: fvcore\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5-py3-none-any.whl size=65155 sha256=5235849b69c8812c9d8b17e4c29ee0098e03130ce6e5f1292a13ac51e13abfbb\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7u24rgwh/wheels/24/1d/09/8167de727fe5b74f832b6fcb5d9069d8f03ca29f337bfe484d\n",
      "Successfully built fvcore\n",
      "Installing collected packages: iopath, fvcore\n",
      "Successfully installed fvcore-0.1.5 iopath-0.1.9\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/facebookresearch/fvcore.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac3da3cc-b62e-4faf-bc76-304917787f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kaggle configuration\n",
    "!mv kaggle-2.json kaggle.json\n",
    "!mv kaggle.json ./.kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d367a2da-dc83-4fb6-9e19-53f6166c6869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10 in /opt/conda/lib/python3.7/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kaggle) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from kaggle) (2.27.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from kaggle) (4.63.0)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.7/site-packages (from kaggle) (6.1.1)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.7/site-packages (from kaggle) (1.26.8)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.7/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->kaggle) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->kaggle) (2.0.12)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=c748fc700eb0c30c70283d5858dd903a0294a64ff24c4bc7be5746028e1dcd95\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "Successfully installed kaggle-1.5.12\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2df80ae4-d35e-41fc-b05c-b955a31d3801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/jupyter/.kaggle/kaggle.json'\n",
      "Output file downloaded to ./input/vilioexample/new_req.txt\n",
      "Kernel log downloaded to ./input/vilioexample/vilioexample-nb.log \n"
     ]
    }
   ],
   "source": [
    "!mkdir input\n",
    "!kaggle kernels output muennighoff/vilioexample-nb -p ./input/vilioexample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98242c0f-c999-4c78-939c-4da058104680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/jupyter/.kaggle/kaggle.json'\n",
      "Downloading vilioexample.zip to /home/jupyter\n",
      "  0%|                                                | 0.00/106k [00:00<?, ?B/s]\n",
      "100%|████████████████████████████████████████| 106k/106k [00:00<00:00, 51.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download muennighoff/vilioexample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fea89ba-3b0a-4a71-b8c1-12d137d3b7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv vilioexample.zip ./input/vilioexample/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a762291d-c027-474f-9922-f5d607e30562",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv vilioexample ./input/vilioexample/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "closing-blowing",
   "metadata": {
    "papermill": {
     "duration": 0.51451,
     "end_time": "2021-06-24T08:04:03.882634",
     "exception": false,
     "start_time": "2021-06-24T08:04:03.368124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting new_req.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile new_req.txt\n",
    "sacremoses==0.0.43\n",
    "pandas==1.1.3\n",
    "regex==2020.4.4\n",
    "h5py==2.10.0\n",
    "filelock==3.0.10\n",
    "scipy==1.4.1\n",
    "sentencepiece~=0.1.91\n",
    "matplotlib==3.2.1\n",
    "torch==1.6.0\n",
    "tensorflow==2.3.1\n",
    "tqdm==4.45.0\n",
    "numpy==1.18.1\n",
    "six==1.14.0\n",
    "packaging==20.1\n",
    "wandb==0.10.8\n",
    "psutil==5.7.0\n",
    "requests==2.23.0\n",
    "pytorch_lightning==1.0.4\n",
    "ImageHash==4.1.0\n",
    "tokenizers~=0.9.2\n",
    "transformers==3.5.1 # Required due to some imports in the files under src/vilio/transformers\n",
    "torchvision==0.7.0\n",
    "jieba==0.42.1\n",
    "botocore==1.19.8\n",
    "spacy==2.3.2\n",
    "boto3==1.16.8\n",
    "comet_ml==3.2.5\n",
    "dataclasses==0.6\n",
    "fairseq==0.9.0\n",
    "ftfy==5.8\n",
    "fugashi==1.0.5\n",
    "ipadic==1.0.0\n",
    "lmdb==1.0.0\n",
    "Pillow==8.0.1\n",
    "py3nvml==0.2.6\n",
    "pydantic==1.7.2\n",
    "pythainlp==2.2.4\n",
    "PyYAML==5.3.1\n",
    "scikit_learn==0.23.2\n",
    "tensorboardX==2.1\n",
    "timeout_decorator==0.4.1\n",
    "torchcontrib==0.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "weighted-rider",
   "metadata": {
    "papermill": {
     "duration": 288.658515,
     "end_time": "2021-06-24T08:08:53.036884",
     "exception": false,
     "start_time": "2021-06-24T08:04:04.378369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacremoses==0.0.43 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 1)) (0.0.43)\n",
      "Collecting pandas==1.1.3\n",
      "  Using cached pandas-1.1.3-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
      "Requirement already satisfied: regex==2020.4.4 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 3)) (2020.4.4)\n",
      "Requirement already satisfied: h5py==2.10.0 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 4)) (2.10.0)\n",
      "Requirement already satisfied: filelock==3.0.10 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 5)) (3.0.10)\n",
      "Collecting scipy==1.4.1\n",
      "  Using cached scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
      "Requirement already satisfied: sentencepiece~=0.1.91 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 7)) (0.1.91)\n",
      "Requirement already satisfied: matplotlib==3.2.1 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 8)) (3.2.1)\n",
      "Collecting torch==1.6.0\n",
      "  Using cached torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8 MB)\n",
      "Requirement already satisfied: tensorflow==2.3.1 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 10)) (2.3.1)\n",
      "Requirement already satisfied: tqdm==4.45.0 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 11)) (4.45.0)\n",
      "Requirement already satisfied: numpy==1.18.1 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 12)) (1.18.1)\n",
      "Collecting six==1.14.0\n",
      "  Using cached six-1.14.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: packaging==20.1 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 14)) (20.1)\n",
      "Requirement already satisfied: wandb==0.10.8 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 15)) (0.10.8)\n",
      "Requirement already satisfied: psutil==5.7.0 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 16)) (5.7.0)\n",
      "Requirement already satisfied: requests==2.23.0 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 17)) (2.23.0)\n",
      "Requirement already satisfied: pytorch_lightning==1.0.4 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 18)) (1.0.4)\n",
      "Requirement already satisfied: ImageHash==4.1.0 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 19)) (4.1.0)\n",
      "Requirement already satisfied: tokenizers~=0.9.2 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 20)) (0.9.3)\n",
      "Requirement already satisfied: transformers==3.5.1 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 21)) (3.5.1)\n",
      "Collecting torchvision==0.7.0\n",
      "  Using cached torchvision-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.9 MB)\n",
      "Requirement already satisfied: jieba==0.42.1 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 23)) (0.42.1)\n",
      "Requirement already satisfied: botocore==1.19.8 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 24)) (1.19.8)\n",
      "Requirement already satisfied: spacy==2.3.2 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 25)) (2.3.2)\n",
      "Requirement already satisfied: boto3==1.16.8 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 26)) (1.16.8)\n",
      "Requirement already satisfied: comet_ml==3.2.5 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 27)) (3.2.5)\n",
      "Requirement already satisfied: dataclasses==0.6 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 28)) (0.6)\n",
      "Requirement already satisfied: fairseq==0.9.0 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 29)) (0.9.0)\n",
      "Requirement already satisfied: ftfy==5.8 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 30)) (5.8)\n",
      "Requirement already satisfied: fugashi==1.0.5 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 31)) (1.0.5)\n",
      "Requirement already satisfied: ipadic==1.0.0 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 32)) (1.0.0)\n",
      "Collecting lmdb==1.0.0\n",
      "  Using cached lmdb-1.0.0-cp37-cp37m-manylinux1_x86_64.whl (280 kB)\n",
      "Requirement already satisfied: Pillow==8.0.1 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 34)) (8.0.1)\n",
      "Requirement already satisfied: py3nvml==0.2.6 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 35)) (0.2.6)\n",
      "Requirement already satisfied: pydantic==1.7.2 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 36)) (1.7.2)\n",
      "Requirement already satisfied: pythainlp==2.2.4 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 37)) (2.2.4)\n",
      "Requirement already satisfied: PyYAML==5.3.1 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 38)) (5.3.1)\n",
      "Requirement already satisfied: scikit_learn==0.23.2 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 39)) (0.23.2)\n",
      "Requirement already satisfied: tensorboardX==2.1 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 40)) (2.1)\n",
      "Requirement already satisfied: timeout_decorator==0.4.1 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 41)) (0.4.1)\n",
      "Requirement already satisfied: torchcontrib==0.0.2 in /opt/conda/lib/python3.7/site-packages (from -r new_req.txt (line 42)) (0.0.2)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses==0.0.43->-r new_req.txt (line 1)) (8.0.4)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses==0.0.43->-r new_req.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.1.3->-r new_req.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas==1.1.3->-r new_req.txt (line 2)) (2021.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.2.1->-r new_req.txt (line 8)) (3.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.2.1->-r new_req.txt (line 8)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib==3.2.1->-r new_req.txt (line 8)) (1.4.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch==1.6.0->-r new_req.txt (line 9)) (0.18.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.1->-r new_req.txt (line 10)) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.1->-r new_req.txt (line 10)) (1.14.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.1->-r new_req.txt (line 10)) (0.37.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages/absl_py-1.0.0-py3.7.egg (from tensorflow==2.3.1->-r new_req.txt (line 10)) (1.0.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.1->-r new_req.txt (line 10)) (2.3.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.1->-r new_req.txt (line 10)) (1.44.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.1->-r new_req.txt (line 10)) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.1->-r new_req.txt (line 10)) (1.1.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.1->-r new_req.txt (line 10)) (0.3.3)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /opt/conda/lib/python3.7/site-packages/tensorboard-2.9.0-py3.7.egg (from tensorflow==2.3.1->-r new_req.txt (line 10)) (2.9.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.1->-r new_req.txt (line 10)) (1.6.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.1->-r new_req.txt (line 10)) (3.19.4)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.3.1->-r new_req.txt (line 10)) (1.1.2)\n",
      "Requirement already satisfied: subprocess32>=3.5.3 in /opt/conda/lib/python3.7/site-packages (from wandb==0.10.8->-r new_req.txt (line 15)) (3.5.4)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb==0.10.8->-r new_req.txt (line 15)) (0.4.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb==0.10.8->-r new_req.txt (line 15)) (3.1.27)\n",
      "Requirement already satisfied: watchdog>=0.8.3 in /opt/conda/lib/python3.7/site-packages (from wandb==0.10.8->-r new_req.txt (line 15)) (2.1.7)\n",
      "Requirement already satisfied: configparser>=3.8.1 in /opt/conda/lib/python3.7/site-packages (from wandb==0.10.8->-r new_req.txt (line 15)) (5.2.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb==0.10.8->-r new_req.txt (line 15)) (1.0.8)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb==0.10.8->-r new_req.txt (line 15)) (2.3)\n",
      "Requirement already satisfied: sentry-sdk>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb==0.10.8->-r new_req.txt (line 15)) (1.5.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests==2.23.0->-r new_req.txt (line 17)) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests==2.23.0->-r new_req.txt (line 17)) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests==2.23.0->-r new_req.txt (line 17)) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests==2.23.0->-r new_req.txt (line 17)) (3.0.4)\n",
      "Requirement already satisfied: fsspec>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning==1.0.4->-r new_req.txt (line 18)) (2022.2.0)\n",
      "Requirement already satisfied: PyWavelets in /opt/conda/lib/python3.7/site-packages (from ImageHash==4.1.0->-r new_req.txt (line 19)) (1.3.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from botocore==1.19.8->-r new_req.txt (line 24)) (0.10.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2->-r new_req.txt (line 25)) (7.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2->-r new_req.txt (line 25)) (1.1.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2->-r new_req.txt (line 25)) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2->-r new_req.txt (line 25)) (0.9.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2->-r new_req.txt (line 25)) (3.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2->-r new_req.txt (line 25)) (2.0.6)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2->-r new_req.txt (line 25)) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2->-r new_req.txt (line 25)) (1.0.7)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2->-r new_req.txt (line 25)) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2->-r new_req.txt (line 25)) (59.8.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from boto3==1.16.8->-r new_req.txt (line 26)) (0.3.7)\n",
      "Requirement already satisfied: everett[ini]>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from comet_ml==3.2.5->-r new_req.txt (line 27)) (3.0.0)\n",
      "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from comet_ml==3.2.5->-r new_req.txt (line 27)) (4.4.0)\n",
      "Requirement already satisfied: dulwich>=0.20.6 in /opt/conda/lib/python3.7/site-packages (from comet_ml==3.2.5->-r new_req.txt (line 27)) (0.20.35)\n",
      "Requirement already satisfied: netifaces>=0.10.7 in /opt/conda/lib/python3.7/site-packages (from comet_ml==3.2.5->-r new_req.txt (line 27)) (0.11.0)\n",
      "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /opt/conda/lib/python3.7/site-packages (from comet_ml==3.2.5->-r new_req.txt (line 27)) (7.352.0)\n",
      "Requirement already satisfied: websocket-client>=0.55.0 in /opt/conda/lib/python3.7/site-packages (from comet_ml==3.2.5->-r new_req.txt (line 27)) (1.3.1)\n",
      "Requirement already satisfied: wurlitzer>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from comet_ml==3.2.5->-r new_req.txt (line 27)) (3.0.2)\n",
      "Requirement already satisfied: cython in /opt/conda/lib/python3.7/site-packages (from fairseq==0.9.0->-r new_req.txt (line 29)) (0.29.28)\n",
      "Requirement already satisfied: cffi in /opt/conda/lib/python3.7/site-packages (from fairseq==0.9.0->-r new_req.txt (line 29)) (1.15.0)\n",
      "Requirement already satisfied: sacrebleu in /opt/conda/lib/python3.7/site-packages (from fairseq==0.9.0->-r new_req.txt (line 29)) (2.0.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from ftfy==5.8->-r new_req.txt (line 30)) (0.2.5)\n",
      "Requirement already satisfied: xmltodict in /opt/conda/lib/python3.7/site-packages (from py3nvml==0.2.6->-r new_req.txt (line 35)) (0.12.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from pythainlp==2.2.4->-r new_req.txt (line 37)) (0.9.8)\n",
      "Requirement already satisfied: tinydb>=3.0 in /opt/conda/lib/python3.7/site-packages (from pythainlp==2.2.4->-r new_req.txt (line 37)) (4.7.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn==0.23.2->-r new_req.txt (line 39)) (3.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3.2->-r new_req.txt (line 25)) (4.11.3)\n",
      "Requirement already satisfied: configobj in /opt/conda/lib/python3.7/site-packages (from everett[ini]>=1.0.1->comet_ml==3.2.5->-r new_req.txt (line 27)) (5.0.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb==0.10.8->-r new_req.txt (line 15)) (4.1.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb==0.10.8->-r new_req.txt (line 15)) (4.0.9)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml==3.2.5->-r new_req.txt (line 27)) (0.18.1)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml==3.2.5->-r new_req.txt (line 27)) (5.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml==3.2.5->-r new_req.txt (line 27)) (21.4.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r new_req.txt (line 10)) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r new_req.txt (line 10)) (2.6.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r new_req.txt (line 10)) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages/tensorboard_data_server-0.6.1-py3.7.egg (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r new_req.txt (line 10)) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages/tensorboard_plugin_wit-1.8.1-py3.7.egg (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r new_req.txt (line 10)) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages/Werkzeug-2.1.2-py3.7.egg (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r new_req.txt (line 10)) (2.1.2)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi->fairseq==0.9.0->-r new_req.txt (line 29)) (2.21)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.7/site-packages (from sacrebleu->fairseq==0.9.0->-r new_req.txt (line 29)) (0.8.9)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from sacrebleu->fairseq==0.9.0->-r new_req.txt (line 29)) (0.4.4)\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from sacrebleu->fairseq==0.9.0->-r new_req.txt (line 29)) (2.4.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb==0.10.8->-r new_req.txt (line 15)) (3.0.5)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r new_req.txt (line 10)) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r new_req.txt (line 10)) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r new_req.txt (line 10)) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r new_req.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.2->-r new_req.txt (line 25)) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r new_req.txt (line 10)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r new_req.txt (line 10)) (3.2.0)\n",
      "Installing collected packages: lmdb, torch, six, scipy, torchvision, pandas\n",
      "  Attempting uninstall: lmdb\n",
      "    Found existing installation: lmdb 0.97\n",
      "    Uninstalling lmdb-0.97:\n",
      "      Successfully uninstalled lmdb-0.97\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.4.0\n",
      "    Uninstalling torch-1.4.0:\n",
      "      Successfully uninstalled torch-1.4.0\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.11.0\n",
      "    Uninstalling six-1.11.0:\n",
      "      Successfully uninstalled six-1.11.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.2.1\n",
      "    Uninstalling scipy-1.2.1:\n",
      "      Successfully uninstalled scipy-1.2.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.5.0\n",
      "    Uninstalling torchvision-0.5.0:\n",
      "      Successfully uninstalled torchvision-0.5.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.5\n",
      "    Uninstalling pandas-1.0.5:\n",
      "      Successfully uninstalled pandas-1.0.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "statsmodels 0.13.2 requires packaging>=21.3, but you have packaging 20.1 which is incompatible.\n",
      "phik 0.12.0 requires scipy>=1.5.2, but you have scipy 1.4.1 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires pydantic>=1.8.1, but you have pydantic 1.7.2 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires requests>=2.24.0, but you have requests 2.23.0 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires tangled-up-in-unicode==0.1.0, but you have tangled-up-in-unicode 0.2.0 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires tqdm>=4.48.2, but you have tqdm 4.45.0 which is incompatible.\n",
      "paddlepaddle-gpu 1.8.3.post97 requires scipy<=1.3.1; python_version >= \"3.5\", but you have scipy 1.4.1 which is incompatible.\n",
      "kubernetes 23.3.0 requires pyyaml>=5.4.1, but you have pyyaml 5.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed lmdb-1.0.0 pandas-1.1.3 scipy-1.4.1 six-1.14.0 torch-1.6.0 torchvision-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r new_req.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "medium-conjunction",
   "metadata": {
    "papermill": {
     "duration": 2.86228,
     "end_time": "2021-06-24T08:08:57.450052",
     "exception": false,
     "start_time": "2021-06-24T08:08:54.587772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat './vilio/py*/data/example.jsonl': No such file or directory\n",
      "cp: cannot stat './vilio/py*/data/hm_vgattr3636.tsv': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#original\n",
    "!cp ./vilio/py*/data/example.jsonl ./vilio/data/\n",
    "!cp ./vilio/py*/data/hm_vgattr3636.tsv ./vilio/data/HM_img.tsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7e37b0d-5265-4db2-b141-dcf9bcae202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "!cp ../vilio/py*/data/test.jsonl ../vilio/data/\n",
    "!cp ../vilio/py*/data/hm_vgattr3636.tsv ../vilio/data/HM_img.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "spoken-characteristic",
   "metadata": {
    "papermill": {
     "duration": 2.280236,
     "end_time": "2021-06-24T08:09:01.025238",
     "exception": false,
     "start_time": "2021-06-24T08:08:58.745002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb\tnew_req.txt  tutorials\tvilioexample-nb.ipynb\n",
      "input\t\tsrc\t     vilio\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42555647-5143-4e7c-a6a2-1ef3fa5af9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/vilio\n"
     ]
    }
   ],
   "source": [
    "%cd vilio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sized-handling",
   "metadata": {
    "papermill": {
     "duration": 1.350065,
     "end_time": "2021-06-24T08:09:03.680431",
     "exception": false,
     "start_time": "2021-06-24T08:09:02.330366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../vilio/hm_inf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../vilio/hm_inf.py\n",
    "import collections\n",
    "import os\n",
    "\n",
    "from param import args\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "if args.tsv:\n",
    "    from fts_tsv.hm_data_tsv import HMTorchDataset, HMEvaluator, HMDataset\n",
    "else:\n",
    "    from fts_lmdb.hm_data import HMTorchDataset, HMEvaluator, HMDataset\n",
    "\n",
    "from src.vilio.transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "from utils.pandas_scripts import clean_data\n",
    "\n",
    "from entryU import ModelU\n",
    "from entryX import ModelX\n",
    "from entryV import ModelV\n",
    "from entryD import ModelD\n",
    "from entryO import ModelO\n",
    "\n",
    "# Two different SWA Methods - https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/\n",
    "if args.swa:\n",
    "    from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "    from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "if args.contrib:\n",
    "    from torchcontrib.optim import SWA\n",
    "\n",
    "\n",
    "# Largely sticking to standards set in LXMERT here\n",
    "DataTuple = collections.namedtuple(\"DataTuple\", 'dataset loader evaluator')\n",
    "\n",
    "def get_tuple(splits: str, bs:int, shuffle=False, drop_last=False) -> DataTuple:\n",
    "\n",
    "    dset =  HMDataset(splits)\n",
    "\n",
    "    tset = HMTorchDataset(splits)\n",
    "    evaluator = HMEvaluator(tset)\n",
    "    data_loader = DataLoader(\n",
    "        tset, batch_size=bs,\n",
    "        shuffle=shuffle, num_workers=args.num_workers,\n",
    "        drop_last=drop_last, pin_memory=True\n",
    "    )\n",
    "\n",
    "    return DataTuple(dataset=dset, loader=data_loader, evaluator=evaluator)\n",
    "\n",
    "class HM:\n",
    "    def __init__(self):\n",
    "        \n",
    "        if args.train is not None:\n",
    "            self.train_tuple = get_tuple(\n",
    "                args.train, bs=args.batch_size, shuffle=True, drop_last=False\n",
    "            )\n",
    "\n",
    "        if args.valid is not None:\n",
    "            valid_bsize = 2048 if args.multiGPU else 50\n",
    "            self.valid_tuple = get_tuple(\n",
    "                args.valid, bs=valid_bsize,\n",
    "                shuffle=False, drop_last=False\n",
    "            )\n",
    "        else:\n",
    "            self.valid_tuple = None\n",
    "\n",
    "        # Select Model, X is default\n",
    "        if args.model == \"X\":\n",
    "            self.model = ModelX(args)\n",
    "        elif args.model == \"V\":\n",
    "            self.model = ModelV(args)\n",
    "        elif args.model == \"U\":\n",
    "            self.model = ModelU(args)\n",
    "        elif args.model == \"D\":\n",
    "            self.model = ModelD(args)\n",
    "        elif args.model == 'O':\n",
    "            self.model = ModelO(args)\n",
    "        else:\n",
    "            print(args.model, \" is not implemented.\")\n",
    "\n",
    "        # Load pre-trained weights from paths\n",
    "        if args.loadpre is not None:\n",
    "            self.model.load(args.loadpre)\n",
    "\n",
    "        # GPU options\n",
    "        if args.multiGPU:\n",
    "            self.model.lxrt_encoder.multi_gpu()\n",
    "\n",
    "        self.model = self.model.cuda()\n",
    "\n",
    "        # Losses and optimizer\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        self.nllloss = nn.NLLLoss()\n",
    "\n",
    "        if args.train is not None:\n",
    "            batch_per_epoch = len(self.train_tuple.loader)\n",
    "            self.t_total = int(batch_per_epoch * args.epochs // args.acc)\n",
    "            print(\"Total Iters: %d\" % self.t_total)\n",
    "\n",
    "        def is_backbone(n):\n",
    "            if \"encoder\" in n:\n",
    "                return True\n",
    "            elif \"embeddings\" in n:\n",
    "                return True\n",
    "            elif \"pooler\" in n:\n",
    "                return True\n",
    "            print(\"F: \", n)\n",
    "            return False\n",
    "\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "        params = list(self.model.named_parameters())\n",
    "        if args.reg:\n",
    "            optimizer_grouped_parameters = [\n",
    "                {\"params\": [p for n, p in params if is_backbone(n)], \"lr\": args.lr},\n",
    "                {\"params\": [p for n, p in params if not is_backbone(n)], \"lr\": args.lr * 500},\n",
    "            ]\n",
    "\n",
    "            for n, p in self.model.named_parameters():\n",
    "                print(n)\n",
    "\n",
    "            self.optim = AdamW(optimizer_grouped_parameters, lr=args.lr)\n",
    "        else:\n",
    "            optimizer_grouped_parameters = [\n",
    "                {'params': [p for n, p in params if not any(nd in n for nd in no_decay)], 'weight_decay': args.wd},\n",
    "                {'params': [p for n, p in params if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "                ]\n",
    "\n",
    "            self.optim = AdamW(optimizer_grouped_parameters, lr=args.lr)\n",
    "\n",
    "        if args.train is not None:\n",
    "            self.scheduler = get_linear_schedule_with_warmup(self.optim, self.t_total * 0.1, self.t_total)\n",
    "        \n",
    "        self.output = args.output\n",
    "        os.makedirs(self.output, exist_ok=True)\n",
    "\n",
    "        # SWA Method:\n",
    "        if args.contrib:\n",
    "            self.optim = SWA(self.optim, swa_start=self.t_total * 0.75, swa_freq=5, swa_lr=args.lr)\n",
    "\n",
    "        if args.swa: \n",
    "            self.swa_model = AveragedModel(self.model)\n",
    "            self.swa_start = self.t_total * 0.75\n",
    "            self.swa_scheduler = SWALR(self.optim, swa_lr=args.lr)\n",
    "\n",
    "    def train(self, train_tuple, eval_tuple):\n",
    "\n",
    "        dset, loader, evaluator = train_tuple\n",
    "        iter_wrapper = (lambda x: tqdm(x, total=len(loader))) if args.tqdm else (lambda x: x)\n",
    "\n",
    "        print(\"Batches:\", len(loader))\n",
    "\n",
    "        self.optim.zero_grad()\n",
    "\n",
    "        best_roc = 0.\n",
    "        ups = 0\n",
    "        \n",
    "        total_loss = 0.\n",
    "\n",
    "        for epoch in range(args.epochs):\n",
    "            \n",
    "            if args.reg:\n",
    "                if args.model != \"X\":\n",
    "                    print(self.model.model.layer_weights)\n",
    "\n",
    "            id2ans = {}\n",
    "            id2prob = {}\n",
    "\n",
    "            for i, (ids, feats, boxes, sent, target) in iter_wrapper(enumerate(loader)):\n",
    "\n",
    "                if ups == args.midsave:\n",
    "                    self.save(\"MID\")\n",
    "\n",
    "                self.model.train()\n",
    "\n",
    "                if args.swa:\n",
    "                    self.swa_model.train()\n",
    "                \n",
    "                feats, boxes, target = feats.cuda(), boxes.cuda(), target.long().cuda()\n",
    "\n",
    "                # Model expects visual feats as tuple of feats & boxes\n",
    "                logit = self.model(sent, (feats, boxes))\n",
    "\n",
    "                # Note: LogSoftmax does not change order, hence there should be nothing wrong with taking it as our prediction \n",
    "                # In fact ROC AUC stays the exact same for logsoftmax / normal softmax, but logsoftmax is better for loss calculation\n",
    "                # due to stronger penalization & decomplexifying properties (log(a/b) = log(a) - log(b))\n",
    "                logit = self.logsoftmax(logit)\n",
    "                score = logit[:, 1]\n",
    "\n",
    "                if i < 1:\n",
    "                    print(logit[0, :].detach())\n",
    "               \n",
    "                # Note: This loss is the same as CrossEntropy (We splitted it up in logsoftmax & neg. log likelihood loss)\n",
    "                loss = self.nllloss(logit.view(-1, 2), target.view(-1))\n",
    "\n",
    "                # Scaling loss by batch size, as we have batches with different sizes, since we do not \"drop_last\" & dividing by acc for accumulation\n",
    "                # Not scaling the loss will worsen performance by ~2abs%\n",
    "                loss = loss * logit.size(0) / args.acc\n",
    "                loss.backward()\n",
    "\n",
    "                total_loss += loss.detach().item()\n",
    "\n",
    "                # Acts as argmax - extracting the higher score & the corresponding index (0 or 1)\n",
    "                _, predict = logit.detach().max(1)\n",
    "                # Getting labels for accuracy\n",
    "                for qid, l in zip(ids, predict.cpu().numpy()):\n",
    "                    id2ans[qid] = l\n",
    "                # Getting probabilities for Roc auc\n",
    "                for qid, l in zip(ids, score.detach().cpu().numpy()):\n",
    "                    id2prob[qid] = l\n",
    "\n",
    "                if (i+1) % args.acc == 0:\n",
    "\n",
    "                    nn.utils.clip_grad_norm_(self.model.parameters(), args.clip)\n",
    "\n",
    "                    self.optim.step()\n",
    "\n",
    "                    if (args.swa) and (ups > self.swa_start):\n",
    "                        self.swa_model.update_parameters(self.model)\n",
    "                        self.swa_scheduler.step()\n",
    "                    else:\n",
    "                        self.scheduler.step()\n",
    "                    self.optim.zero_grad()\n",
    "\n",
    "                    ups += 1\n",
    "\n",
    "                    # Do Validation in between\n",
    "                    if ups % 250 == 0: \n",
    "                        \n",
    "                        log_str = \"\\nEpoch(U) %d(%d): Train AC %0.2f RA %0.4f LOSS %0.4f\\n\" % (epoch, ups, evaluator.evaluate(id2ans)*100, \n",
    "                        evaluator.roc_auc(id2prob)*100, total_loss)\n",
    "\n",
    "                        # Set loss back to 0 after printing it\n",
    "                        total_loss = 0.\n",
    "\n",
    "                        if self.valid_tuple is not None:  # Do Validation\n",
    "                            acc, roc_auc = self.evaluate(eval_tuple)\n",
    "                            if roc_auc > best_roc:\n",
    "                                best_roc = roc_auc\n",
    "                                best_acc = acc\n",
    "                                # Only save BEST when no midsave is specified to save space\n",
    "                                #if args.midsave < 0:\n",
    "                                #    self.save(\"BEST\")\n",
    "\n",
    "                            log_str += \"\\nEpoch(U) %d(%d): DEV AC %0.2f RA %0.4f \\n\" % (epoch, ups, acc*100.,roc_auc*100)\n",
    "                            log_str += \"Epoch(U) %d(%d): BEST AC %0.2f RA %0.4f \\n\" % (epoch, ups, best_acc*100., best_roc*100.)\n",
    "    \n",
    "                        print(log_str, end='')\n",
    "\n",
    "                        with open(self.output + \"/log.log\", 'a') as f:\n",
    "                            f.write(log_str)\n",
    "                            f.flush()\n",
    "\n",
    "        if (epoch + 1) == args.epochs:\n",
    "            if args.contrib:\n",
    "                self.optim.swap_swa_sgd()\n",
    "\n",
    "        self.save(\"LAST\" + args.train)\n",
    "\n",
    "    def predict(self, eval_tuple: DataTuple, dump=None, out_csv=True):\n",
    "\n",
    "        dset, loader, evaluator = eval_tuple\n",
    "        id2ans = {}\n",
    "        id2prob = {}\n",
    "\n",
    "        for i, datum_tuple in enumerate(loader):\n",
    "\n",
    "            ids, feats, boxes, sent = datum_tuple[:4]\n",
    "\n",
    "            self.model.eval()\n",
    "\n",
    "            if args.swa:\n",
    "                self.swa_model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                feats, boxes = feats.cuda(), boxes.cuda()\n",
    "                logit = self.model(sent, (feats, boxes))\n",
    "\n",
    "                # Note: LogSoftmax does not change order, hence there should be nothing wrong with taking it as our prediction\n",
    "                logit = self.logsoftmax(logit)\n",
    "                score = logit[:, 1]\n",
    "\n",
    "                if args.swa:\n",
    "                    logit = self.swa_model(sent, (feats, boxes))\n",
    "                    logit = self.logsoftmax(logit)\n",
    "\n",
    "                _, predict = logit.max(1)\n",
    "\n",
    "                for qid, l in zip(ids, predict.cpu().numpy()):\n",
    "                    id2ans[qid] = l\n",
    "\n",
    "                # Getting probas for Roc Auc\n",
    "                for qid, l in zip(ids, score.cpu().numpy()):\n",
    "                    id2prob[qid] = l\n",
    "\n",
    "        if dump is not None:\n",
    "            if out_csv == True:\n",
    "                evaluator.dump_csv(id2ans, id2prob, dump)\n",
    "            else:\n",
    "                evaluator.dump_result(id2ans, dump)\n",
    "\n",
    "        return id2ans, id2prob\n",
    "\n",
    "    def evaluate(self, eval_tuple: DataTuple, dump=None):\n",
    "        \"\"\"Evaluate all data in data_tuple.\"\"\"\n",
    "        id2ans, id2prob = self.predict(eval_tuple, dump=dump)\n",
    "\n",
    "        acc = eval_tuple.evaluator.evaluate(id2ans)\n",
    "        roc_auc = eval_tuple.evaluator.roc_auc(id2prob)\n",
    "\n",
    "        return acc, roc_auc\n",
    "\n",
    "    def save(self, name):\n",
    "        if args.swa:\n",
    "            torch.save(self.swa_model.state_dict(),\n",
    "                    os.path.join(self.output, \"%s.pth\" % name))\n",
    "        else:\n",
    "            torch.save(self.model.state_dict(),\n",
    "                    os.path.join(self.output, \"%s.pth\" % name))\n",
    "\n",
    "    def load(self, path):\n",
    "        print(\"Load model from %s\" % path)\n",
    "            \n",
    "        state_dict = torch.load(\"%s\" % path)\n",
    "        new_state_dict = {}\n",
    "        for key, value in state_dict.items():\n",
    "            # N_averaged is a key in SWA models we cannot load, so we skip it\n",
    "            if key.startswith(\"n_averaged\"):\n",
    "                print(\"n_averaged:\", value)\n",
    "                continue\n",
    "            # SWA Models will start with module\n",
    "            if key.startswith(\"module.\"):\n",
    "                new_state_dict[key[len(\"module.\"):]] = value\n",
    "            else:\n",
    "                new_state_dict[key] = value\n",
    "        state_dict = new_state_dict\n",
    "        self.model.load_state_dict(state_dict)\n",
    "\n",
    "def main():\n",
    "    # Build Class\n",
    "    hm = HM()\n",
    "\n",
    "    # Load Model\n",
    "    if args.loadfin is not None:\n",
    "        hm.load(args.loadfin)\n",
    "\n",
    "    # Train and/or Test:\n",
    "    if args.train is not None:\n",
    "        print('Splits in Train data:', hm.train_tuple.dataset.splits)\n",
    "        if hm.valid_tuple is not None:\n",
    "            print('Splits in Valid data:', hm.valid_tuple.dataset.splits)\n",
    "        else:\n",
    "            print(\"DO NOT USE VALIDATION\")\n",
    "        hm.train(hm.train_tuple, hm.valid_tuple)\n",
    "\n",
    "        # If we also test afterwards load the last model\n",
    "        if args.test is not None:\n",
    "            hm.load(os.path.join(hm.output, \"LAST\" + args.train + \".pth\"))\n",
    "\n",
    "    if args.test is not None:\n",
    "        # We can specify multiple test args e.g. test,test_unseen\n",
    "        for split in args.test.split(\",\"):\n",
    "            # Anthing that has no labels:\n",
    "            if 'test' in split:\n",
    "                hm.predict(\n",
    "                    get_tuple(split, bs=args.batch_size,\n",
    "                            shuffle=False, drop_last=False),\n",
    "                    dump=os.path.join(args.output, '{}_{}.csv'.format(args.exp, split))\n",
    "                )\n",
    "            # Anything else that has labels:\n",
    "            elif 'dev' in split or 'valid' in split or 'train' in split:\n",
    "                result = hm.evaluate(\n",
    "                    get_tuple(split, bs=args.batch_size,\n",
    "                            shuffle=False, drop_last=False),\n",
    "                    dump=os.path.join(args.output, '{}_{}.csv'.format(args.exp, split))\n",
    "                )\n",
    "                print(result)\n",
    "            # Same as test - assuming that it has no labels\n",
    "            else:\n",
    "                hm.predict(\n",
    "                    get_tuple(split, bs=args.batch_size,\n",
    "                            shuffle=False, drop_last=False),\n",
    "                    dump=os.path.join(args.output, '{}_{}.csv'.format(args.exp, split))\n",
    "                )\n",
    "                \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "considerable-mainland",
   "metadata": {
    "papermill": {
     "duration": 2.290153,
     "end_time": "2021-06-24T08:09:07.524382",
     "exception": false,
     "start_time": "2021-06-24T08:09:05.234229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!cp ../input/testjsonl/example.jsonl ../vilio/data/\n",
    "!cp ../input/vilioexample/vilioexample/example.jsonl ../vilio/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7de11e3-77d3-4ce9-adfe-4cf6bd693f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "\n",
    "!cp ../input/facebook-hateful-meme-dataset/data/dev.jsonl ../vilio/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07ab2ba5-f75e-4090-8a58-75c279a90561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/jupyter/.kaggle/kaggle.json'\n",
      "Downloading viliou36.zip to /home/jupyter/vilio\n",
      "100%|██████████████████████████████████████▉| 9.12G/9.14G [01:27<00:00, 115MB/s]\n",
      "100%|███████████████████████████████████████| 9.14G/9.14G [01:27<00:00, 112MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download muennighoff/viliou36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9472e38-7f52-438e-90e0-5bd721bdaf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/vilio\n"
     ]
    }
   ],
   "source": [
    "%cd ./vilio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41829822-ef97-48e8-b8ab-f9d135d53230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  viliou36.zip\n",
      "  inflating: LASTtrain.pth           \n",
      "  inflating: LASTtrain_s1.pth        \n",
      "  inflating: LASTtrain_s2.pth        \n",
      "  inflating: LASTtrain_s3.pth        \n",
      "  inflating: LASTtraindev.pth        \n",
      "  inflating: LASTtraindev_s1.pth     \n",
      "  inflating: LASTtraindev_s2.pth     \n",
      "  inflating: LASTtraindev_s3.pth     \n"
     ]
    }
   ],
   "source": [
    "!unzip viliou36.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff8c1dc2-04b7-4a8a-a1e0-616e7fb08b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/jupyter/.kaggle/kaggle.json'\n",
      "Downloading facebook-hateful-meme-dataset.zip to /home/jupyter/vilio\n",
      "100%|██████████████████████████████████████▉| 3.35G/3.35G [00:37<00:00, 141MB/s]\n",
      "100%|██████████████████████████████████████| 3.35G/3.35G [00:37<00:00, 96.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download parthplc/facebook-hateful-meme-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07279f50-e236-4b6f-aca7-366f3625661e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/input\n"
     ]
    }
   ],
   "source": [
    "%cd ./input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68c147c6-d019-445a-89e5-bd2ee61834e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd ./facebook-hateful-meme-dataset/\n",
    "# !unzip facebook-hateful-meme-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "atlantic-trout",
   "metadata": {
    "papermill": {
     "duration": 157.924375,
     "end_time": "2021-06-24T08:11:46.763093",
     "exception": false,
     "start_time": "2021-06-24T08:09:08.838718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-07 16:55:35.049899: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-05-07 16:55:35.049940: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "Some weights of BertU were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['bert.img_embeddings.img_linear.weight', 'bert.img_embeddings.img_linear.bias', 'bert.img_embeddings.img_layer_norm.weight', 'bert.img_embeddings.img_layer_norm.bias', 'bert.img_embeddings.pos_layer_norm.weight', 'bert.img_embeddings.pos_layer_norm.bias', 'bert.img_embeddings.pos_linear.weight', 'bert.img_embeddings.pos_linear.bias', 'bert.img_embeddings.LayerNorm.weight', 'bert.img_embeddings.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "UNEXPECTED:  []\n",
      "MISSING:  ['bert.img_embeddings.img_linear.weight', 'bert.img_embeddings.img_linear.bias', 'bert.img_embeddings.img_layer_norm.weight', 'bert.img_embeddings.img_layer_norm.bias', 'bert.img_embeddings.pos_layer_norm.weight', 'bert.img_embeddings.pos_layer_norm.bias', 'bert.img_embeddings.pos_linear.weight', 'bert.img_embeddings.pos_linear.bias', 'bert.img_embeddings.LayerNorm.weight', 'bert.img_embeddings.LayerNorm.bias']\n",
      "ERRORS:  []\n",
      "REINITING:  Linear(in_features=1024, out_features=2048, bias=True)\n",
      "REINITING:  GeLU()\n",
      "REINITING:  LayerNorm((2048,), eps=1e-12, elementwise_affine=True)\n",
      "REINITING:  Linear(in_features=2048, out_features=2, bias=True)\n",
      "REINITING:  Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (1): GeLU()\n",
      "  (2): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)\n",
      "  (3): Linear(in_features=2048, out_features=2, bias=True)\n",
      ")\n",
      "Load model from ./data/LASTtrain.pth\n",
      "Load 1000 data from split(s) test_unseen.\n",
      "Start to load Faster-RCNN detected objects from data/HM_img.tsv\n",
      "Loaded 1000 images in file data/HM_img.tsv in 35 seconds.\n",
      "Use 1000 data in torch dataset\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   id      1000 non-null   int64  \n",
      " 1   proba   1000 non-null   float64\n",
      " 2   label   1000 non-null   int64  \n",
      "dtypes: float64(1), int64(2)\n",
      "memory usage: 23.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "!cd ../vilio; python hm_inf.py --seed 42 --model U \\\n",
    "--test test_unseen --batchSize 1 --tr bert-large-cased --tsv \\\n",
    "--num_features 36 --num_pos 6 --exp U36 --loadfin ./data/LASTtrain.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ae9cf6-79d6-4028-8e27-babbedcce75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-08 00:41:30.390844: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-05-08 00:41:30.390888: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "Some weights of BertU were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['bert.img_embeddings.img_linear.weight', 'bert.img_embeddings.img_linear.bias', 'bert.img_embeddings.img_layer_norm.weight', 'bert.img_embeddings.img_layer_norm.bias', 'bert.img_embeddings.pos_layer_norm.weight', 'bert.img_embeddings.pos_layer_norm.bias', 'bert.img_embeddings.pos_linear.weight', 'bert.img_embeddings.pos_linear.bias', 'bert.img_embeddings.LayerNorm.weight', 'bert.img_embeddings.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "UNEXPECTED:  []\n",
      "MISSING:  ['bert.img_embeddings.img_linear.weight', 'bert.img_embeddings.img_linear.bias', 'bert.img_embeddings.img_layer_norm.weight', 'bert.img_embeddings.img_layer_norm.bias', 'bert.img_embeddings.pos_layer_norm.weight', 'bert.img_embeddings.pos_layer_norm.bias', 'bert.img_embeddings.pos_linear.weight', 'bert.img_embeddings.pos_linear.bias', 'bert.img_embeddings.LayerNorm.weight', 'bert.img_embeddings.LayerNorm.bias']\n",
      "ERRORS:  []\n",
      "REINITING:  Linear(in_features=1024, out_features=2048, bias=True)\n",
      "REINITING:  GeLU()\n",
      "REINITING:  LayerNorm((2048,), eps=1e-12, elementwise_affine=True)\n",
      "REINITING:  Linear(in_features=2048, out_features=2, bias=True)\n",
      "REINITING:  Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (1): GeLU()\n",
      "  (2): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)\n",
      "  (3): Linear(in_features=2048, out_features=2, bias=True)\n",
      ")\n",
      "Load model from ./data/LASTtrain.pth\n",
      "Load 500 data from split(s) dev_seen.\n",
      "Start to load Faster-RCNN detected objects from data/HM_img.tsv\n",
      "Loaded 500 images in file data/HM_img.tsv in 35 seconds.\n",
      "Use 500 data in torch dataset\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   id      500 non-null    int64  \n",
      " 1   proba   500 non-null    float64\n",
      " 2   label   500 non-null    int64  \n",
      "dtypes: float64(1), int64(2)\n",
      "memory usage: 11.8 KB\n",
      "None\n",
      "(0.682, 0.7656320000000001)\n"
     ]
    }
   ],
   "source": [
    "!cd ../vilio; python hm_inf.py --seed 42 --model U \\\n",
    "--test dev_seen --batchSize 1 --tr bert-large-cased --tsv \\\n",
    "--num_features 36 --num_pos 6 --exp U36 --loadfin ./data/LASTtrain.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "scheduled-shopping",
   "metadata": {
    "papermill": {
     "duration": 2.73387,
     "end_time": "2021-06-24T08:11:51.190481",
     "exception": false,
     "start_time": "2021-06-24T08:11:48.456611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !cat ../vilio/data/U36_dev.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "refined-benefit",
   "metadata": {
    "papermill": {
     "duration": 1.751099,
     "end_time": "2021-06-24T08:11:54.660959",
     "exception": false,
     "start_time": "2021-06-24T08:11:52.909860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/pdlsp-final-project/uniter-oscar/vilio\n"
     ]
    }
   ],
   "source": [
    "%cd vilio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d042ed-0818-4885-9a1d-e58afc80c30d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 918.713758,
   "end_time": "2021-06-24T08:11:56.986946",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-24T07:56:38.273188",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
