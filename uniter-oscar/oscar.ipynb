{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725cb641-cc4c-4d84-981f-cbcf613f2e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-05-06 17:30:54--  https://biglmdiag.blob.core.windows.net/oscar/pretrained_models/large-vg-labels.zip\n",
      "Resolving biglmdiag.blob.core.windows.net (biglmdiag.blob.core.windows.net)... 52.240.48.36\n",
      "Connecting to biglmdiag.blob.core.windows.net (biglmdiag.blob.core.windows.net)|52.240.48.36|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5024784124 (4.7G) [application/octet-stream]\n",
      "Saving to: ‘large-vg-labels.zip’\n",
      "\n",
      "large-vg-labels.zip 100%[===================>]   4.68G  12.5MB/s    in 6m 43s  \n",
      "\n",
      "2022-05-06 17:37:37 (11.9 MB/s) - ‘large-vg-labels.zip’ saved [5024784124/5024784124]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://biglmdiag.blob.core.windows.net/oscar/pretrained_models/large-vg-labels.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f15b67c-31db-4b8e-a257-320a42f2beba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  large-vg-labels.zip\n",
      "   creating: large-vg-labels/\n",
      "   creating: large-vg-labels/ep_20_590000/\n",
      " extracting: large-vg-labels/ep_20_590000/added_tokens.json  \n",
      "  inflating: large-vg-labels/ep_20_590000/config.json  \n",
      "  inflating: large-vg-labels/ep_20_590000/pytorch_model.bin  \n",
      "  inflating: large-vg-labels/ep_20_590000/special_tokens_map.json  \n",
      "  inflating: large-vg-labels/ep_20_590000/vocab.txt  \n",
      "   creating: large-vg-labels/ep_34_999600/\n",
      " extracting: large-vg-labels/ep_34_999600/added_tokens.json  \n",
      "  inflating: large-vg-labels/ep_34_999600/config.json  \n",
      "  inflating: large-vg-labels/ep_34_999600/pytorch_model.bin  \n",
      "  inflating: large-vg-labels/ep_34_999600/special_tokens_map.json  \n",
      "  inflating: large-vg-labels/ep_34_999600/vocab.txt  \n",
      "   creating: large-vg-labels/ep_55_1617000/\n",
      " extracting: large-vg-labels/ep_55_1617000/added_tokens.json  \n",
      "  inflating: large-vg-labels/ep_55_1617000/config.json  \n",
      "  inflating: large-vg-labels/ep_55_1617000/pytorch_model.bin  \n",
      "  inflating: large-vg-labels/ep_55_1617000/special_tokens_map.json  \n",
      "  inflating: large-vg-labels/ep_55_1617000/vocab.txt  \n",
      "   creating: large-vg-labels/ep_7_816000/\n",
      " extracting: large-vg-labels/ep_7_816000/added_tokens.json  \n",
      "  inflating: large-vg-labels/ep_7_816000/config.json  \n",
      "  inflating: large-vg-labels/ep_7_816000/log.txt  \n",
      "  inflating: large-vg-labels/ep_7_816000/pytorch_model.bin  \n",
      "  inflating: large-vg-labels/ep_7_816000/special_tokens_map.json  \n",
      "  inflating: large-vg-labels/ep_7_816000/vocab.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip large-vg-labels.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b64fb11-0988-44f1-ac51-49ced36f7fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./large-vg-labels/ep_20_590000/pytorch_model.bin ./vilio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "484120ab-cebf-4f41-822c-bdf091785be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-06 20:39:49.222825: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-05-06 20:39:49.222877: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "pretrain\n",
      "Load 8996 data from split(s) pretrain.\n",
      "Start to load Faster-RCNN detected objects from data/HM_img.tsv\n",
      "Loaded 7500 images in file data/HM_img.tsv in 40 seconds.\n",
      "Use 7500 data in torch dataset\n",
      "\n",
      "\n",
      "Some weights of BertOPretraining were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['bert.img_embedding.weight', 'bert.img_embedding.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Load pre-trained model from ./data/pytorch_model.bin\n",
      "MODIFYING: bert.img_embedding.weight\n",
      "\n",
      "Weights in loaded but not in model:\n",
      "\n",
      "Weights in model but not in loaded:\n",
      "bert.embeddings.position_ids\n",
      "\n",
      "Batch per epoch: 468\n",
      "Total Iters: 1872\n",
      "Warm up Iters: 93\n",
      "100%|█████████████████████████████████████████| 468/468 [13:46<00:00,  1.77s/it]\n",
      "The training loss for Epoch 0 is 4.8383\n",
      "The losses are Mask_LM: 3.8560 Matched: 0.9823 \n",
      "100%|█████████████████████████████████████████| 468/468 [13:46<00:00,  1.77s/it]\n",
      "The training loss for Epoch 1 is 4.2487\n",
      "The losses are Mask_LM: 3.5374 Matched: 0.7113 \n",
      "100%|█████████████████████████████████████████| 468/468 [13:46<00:00,  1.77s/it]\n",
      "The training loss for Epoch 2 is 4.1532\n",
      "The losses are Mask_LM: 3.4453 Matched: 0.7079 \n",
      "100%|█████████████████████████████████████████| 468/468 [13:46<00:00,  1.77s/it]\n",
      "The training loss for Epoch 3 is 4.1796\n",
      "The losses are Mask_LM: 3.4726 Matched: 0.7070 \n",
      "2022-05-06 21:36:09.232859: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-05-06 21:36:09.232899: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "Load 7500 data from split(s) train.\n",
      "Start to load Faster-RCNN detected objects from data/HM_img.tsv\n",
      "Loaded 7500 images in file data/HM_img.tsv in 46 seconds.\n",
      "Use 7500 data in torch dataset\n",
      "\n",
      "Load 500 data from split(s) dev_seen.\n",
      "Start to load Faster-RCNN detected objects from data/HM_img.tsv\n",
      "Loaded 500 images in file data/HM_img.tsv in 34 seconds.\n",
      "Use 500 data in torch dataset\n",
      "\n",
      "Some weights of BertO were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['bert.img_embedding.weight', 'bert.img_embedding.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "UNEXPECTED:  []\n",
      "MISSING:  ['bert.img_embedding.weight', 'bert.img_embedding.bias']\n",
      "ERRORS:  []\n",
      "REINITING:  Linear(in_features=1024, out_features=2048, bias=True)\n",
      "REINITING:  GeLU()\n",
      "REINITING:  LayerNorm((2048,), eps=1e-12, elementwise_affine=True)\n",
      "REINITING:  Linear(in_features=2048, out_features=2, bias=True)\n",
      "REINITING:  Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (1): GeLU()\n",
      "  (2): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)\n",
      "  (3): Linear(in_features=2048, out_features=2, bias=True)\n",
      ")\n",
      "Load pre-trained model from ./data/LAST_BO.pth\n",
      "SAVING bert.embeddings.position_ids as embeddings.position_ids.\n",
      "SAVING bert.embeddings.word_embeddings.weight as embeddings.word_embeddings.weight.\n",
      "SAVING bert.embeddings.position_embeddings.weight as embeddings.position_embeddings.weight.\n",
      "SAVING bert.embeddings.token_type_embeddings.weight as embeddings.token_type_embeddings.weight.\n",
      "SAVING bert.embeddings.LayerNorm.weight as embeddings.LayerNorm.weight.\n",
      "SAVING bert.embeddings.LayerNorm.bias as embeddings.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.0.attention.self.query.weight as encoder.layer.0.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.0.attention.self.query.bias as encoder.layer.0.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.0.attention.self.key.weight as encoder.layer.0.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.0.attention.self.key.bias as encoder.layer.0.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.0.attention.self.value.weight as encoder.layer.0.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.0.attention.self.value.bias as encoder.layer.0.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.0.attention.output.dense.weight as encoder.layer.0.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.0.attention.output.dense.bias as encoder.layer.0.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.0.attention.output.LayerNorm.weight as encoder.layer.0.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.0.attention.output.LayerNorm.bias as encoder.layer.0.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.0.intermediate.dense.weight as encoder.layer.0.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.0.intermediate.dense.bias as encoder.layer.0.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.0.output.dense.weight as encoder.layer.0.output.dense.weight.\n",
      "SAVING bert.encoder.layer.0.output.dense.bias as encoder.layer.0.output.dense.bias.\n",
      "SAVING bert.encoder.layer.0.output.LayerNorm.weight as encoder.layer.0.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.0.output.LayerNorm.bias as encoder.layer.0.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.1.attention.self.query.weight as encoder.layer.1.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.1.attention.self.query.bias as encoder.layer.1.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.1.attention.self.key.weight as encoder.layer.1.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.1.attention.self.key.bias as encoder.layer.1.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.1.attention.self.value.weight as encoder.layer.1.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.1.attention.self.value.bias as encoder.layer.1.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.1.attention.output.dense.weight as encoder.layer.1.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.1.attention.output.dense.bias as encoder.layer.1.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.1.attention.output.LayerNorm.weight as encoder.layer.1.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.1.attention.output.LayerNorm.bias as encoder.layer.1.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.1.intermediate.dense.weight as encoder.layer.1.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.1.intermediate.dense.bias as encoder.layer.1.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.1.output.dense.weight as encoder.layer.1.output.dense.weight.\n",
      "SAVING bert.encoder.layer.1.output.dense.bias as encoder.layer.1.output.dense.bias.\n",
      "SAVING bert.encoder.layer.1.output.LayerNorm.weight as encoder.layer.1.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.1.output.LayerNorm.bias as encoder.layer.1.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.2.attention.self.query.weight as encoder.layer.2.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.2.attention.self.query.bias as encoder.layer.2.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.2.attention.self.key.weight as encoder.layer.2.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.2.attention.self.key.bias as encoder.layer.2.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.2.attention.self.value.weight as encoder.layer.2.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.2.attention.self.value.bias as encoder.layer.2.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.2.attention.output.dense.weight as encoder.layer.2.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.2.attention.output.dense.bias as encoder.layer.2.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.2.attention.output.LayerNorm.weight as encoder.layer.2.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.2.attention.output.LayerNorm.bias as encoder.layer.2.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.2.intermediate.dense.weight as encoder.layer.2.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.2.intermediate.dense.bias as encoder.layer.2.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.2.output.dense.weight as encoder.layer.2.output.dense.weight.\n",
      "SAVING bert.encoder.layer.2.output.dense.bias as encoder.layer.2.output.dense.bias.\n",
      "SAVING bert.encoder.layer.2.output.LayerNorm.weight as encoder.layer.2.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.2.output.LayerNorm.bias as encoder.layer.2.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.3.attention.self.query.weight as encoder.layer.3.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.3.attention.self.query.bias as encoder.layer.3.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.3.attention.self.key.weight as encoder.layer.3.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.3.attention.self.key.bias as encoder.layer.3.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.3.attention.self.value.weight as encoder.layer.3.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.3.attention.self.value.bias as encoder.layer.3.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.3.attention.output.dense.weight as encoder.layer.3.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.3.attention.output.dense.bias as encoder.layer.3.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.3.attention.output.LayerNorm.weight as encoder.layer.3.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.3.attention.output.LayerNorm.bias as encoder.layer.3.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.3.intermediate.dense.weight as encoder.layer.3.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.3.intermediate.dense.bias as encoder.layer.3.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.3.output.dense.weight as encoder.layer.3.output.dense.weight.\n",
      "SAVING bert.encoder.layer.3.output.dense.bias as encoder.layer.3.output.dense.bias.\n",
      "SAVING bert.encoder.layer.3.output.LayerNorm.weight as encoder.layer.3.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.3.output.LayerNorm.bias as encoder.layer.3.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.4.attention.self.query.weight as encoder.layer.4.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.4.attention.self.query.bias as encoder.layer.4.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.4.attention.self.key.weight as encoder.layer.4.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.4.attention.self.key.bias as encoder.layer.4.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.4.attention.self.value.weight as encoder.layer.4.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.4.attention.self.value.bias as encoder.layer.4.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.4.attention.output.dense.weight as encoder.layer.4.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.4.attention.output.dense.bias as encoder.layer.4.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.4.attention.output.LayerNorm.weight as encoder.layer.4.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.4.attention.output.LayerNorm.bias as encoder.layer.4.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.4.intermediate.dense.weight as encoder.layer.4.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.4.intermediate.dense.bias as encoder.layer.4.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.4.output.dense.weight as encoder.layer.4.output.dense.weight.\n",
      "SAVING bert.encoder.layer.4.output.dense.bias as encoder.layer.4.output.dense.bias.\n",
      "SAVING bert.encoder.layer.4.output.LayerNorm.weight as encoder.layer.4.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.4.output.LayerNorm.bias as encoder.layer.4.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.5.attention.self.query.weight as encoder.layer.5.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.5.attention.self.query.bias as encoder.layer.5.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.5.attention.self.key.weight as encoder.layer.5.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.5.attention.self.key.bias as encoder.layer.5.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.5.attention.self.value.weight as encoder.layer.5.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.5.attention.self.value.bias as encoder.layer.5.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.5.attention.output.dense.weight as encoder.layer.5.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.5.attention.output.dense.bias as encoder.layer.5.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.5.attention.output.LayerNorm.weight as encoder.layer.5.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.5.attention.output.LayerNorm.bias as encoder.layer.5.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.5.intermediate.dense.weight as encoder.layer.5.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.5.intermediate.dense.bias as encoder.layer.5.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.5.output.dense.weight as encoder.layer.5.output.dense.weight.\n",
      "SAVING bert.encoder.layer.5.output.dense.bias as encoder.layer.5.output.dense.bias.\n",
      "SAVING bert.encoder.layer.5.output.LayerNorm.weight as encoder.layer.5.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.5.output.LayerNorm.bias as encoder.layer.5.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.6.attention.self.query.weight as encoder.layer.6.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.6.attention.self.query.bias as encoder.layer.6.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.6.attention.self.key.weight as encoder.layer.6.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.6.attention.self.key.bias as encoder.layer.6.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.6.attention.self.value.weight as encoder.layer.6.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.6.attention.self.value.bias as encoder.layer.6.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.6.attention.output.dense.weight as encoder.layer.6.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.6.attention.output.dense.bias as encoder.layer.6.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.6.attention.output.LayerNorm.weight as encoder.layer.6.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.6.attention.output.LayerNorm.bias as encoder.layer.6.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.6.intermediate.dense.weight as encoder.layer.6.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.6.intermediate.dense.bias as encoder.layer.6.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.6.output.dense.weight as encoder.layer.6.output.dense.weight.\n",
      "SAVING bert.encoder.layer.6.output.dense.bias as encoder.layer.6.output.dense.bias.\n",
      "SAVING bert.encoder.layer.6.output.LayerNorm.weight as encoder.layer.6.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.6.output.LayerNorm.bias as encoder.layer.6.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.7.attention.self.query.weight as encoder.layer.7.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.7.attention.self.query.bias as encoder.layer.7.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.7.attention.self.key.weight as encoder.layer.7.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.7.attention.self.key.bias as encoder.layer.7.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.7.attention.self.value.weight as encoder.layer.7.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.7.attention.self.value.bias as encoder.layer.7.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.7.attention.output.dense.weight as encoder.layer.7.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.7.attention.output.dense.bias as encoder.layer.7.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.7.attention.output.LayerNorm.weight as encoder.layer.7.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.7.attention.output.LayerNorm.bias as encoder.layer.7.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.7.intermediate.dense.weight as encoder.layer.7.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.7.intermediate.dense.bias as encoder.layer.7.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.7.output.dense.weight as encoder.layer.7.output.dense.weight.\n",
      "SAVING bert.encoder.layer.7.output.dense.bias as encoder.layer.7.output.dense.bias.\n",
      "SAVING bert.encoder.layer.7.output.LayerNorm.weight as encoder.layer.7.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.7.output.LayerNorm.bias as encoder.layer.7.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.8.attention.self.query.weight as encoder.layer.8.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.8.attention.self.query.bias as encoder.layer.8.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.8.attention.self.key.weight as encoder.layer.8.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.8.attention.self.key.bias as encoder.layer.8.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.8.attention.self.value.weight as encoder.layer.8.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.8.attention.self.value.bias as encoder.layer.8.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.8.attention.output.dense.weight as encoder.layer.8.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.8.attention.output.dense.bias as encoder.layer.8.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.8.attention.output.LayerNorm.weight as encoder.layer.8.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.8.attention.output.LayerNorm.bias as encoder.layer.8.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.8.intermediate.dense.weight as encoder.layer.8.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.8.intermediate.dense.bias as encoder.layer.8.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.8.output.dense.weight as encoder.layer.8.output.dense.weight.\n",
      "SAVING bert.encoder.layer.8.output.dense.bias as encoder.layer.8.output.dense.bias.\n",
      "SAVING bert.encoder.layer.8.output.LayerNorm.weight as encoder.layer.8.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.8.output.LayerNorm.bias as encoder.layer.8.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.9.attention.self.query.weight as encoder.layer.9.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.9.attention.self.query.bias as encoder.layer.9.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.9.attention.self.key.weight as encoder.layer.9.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.9.attention.self.key.bias as encoder.layer.9.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.9.attention.self.value.weight as encoder.layer.9.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.9.attention.self.value.bias as encoder.layer.9.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.9.attention.output.dense.weight as encoder.layer.9.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.9.attention.output.dense.bias as encoder.layer.9.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.9.attention.output.LayerNorm.weight as encoder.layer.9.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.9.attention.output.LayerNorm.bias as encoder.layer.9.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.9.intermediate.dense.weight as encoder.layer.9.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.9.intermediate.dense.bias as encoder.layer.9.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.9.output.dense.weight as encoder.layer.9.output.dense.weight.\n",
      "SAVING bert.encoder.layer.9.output.dense.bias as encoder.layer.9.output.dense.bias.\n",
      "SAVING bert.encoder.layer.9.output.LayerNorm.weight as encoder.layer.9.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.9.output.LayerNorm.bias as encoder.layer.9.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.10.attention.self.query.weight as encoder.layer.10.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.10.attention.self.query.bias as encoder.layer.10.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.10.attention.self.key.weight as encoder.layer.10.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.10.attention.self.key.bias as encoder.layer.10.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.10.attention.self.value.weight as encoder.layer.10.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.10.attention.self.value.bias as encoder.layer.10.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.10.attention.output.dense.weight as encoder.layer.10.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.10.attention.output.dense.bias as encoder.layer.10.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.10.attention.output.LayerNorm.weight as encoder.layer.10.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.10.attention.output.LayerNorm.bias as encoder.layer.10.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.10.intermediate.dense.weight as encoder.layer.10.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.10.intermediate.dense.bias as encoder.layer.10.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.10.output.dense.weight as encoder.layer.10.output.dense.weight.\n",
      "SAVING bert.encoder.layer.10.output.dense.bias as encoder.layer.10.output.dense.bias.\n",
      "SAVING bert.encoder.layer.10.output.LayerNorm.weight as encoder.layer.10.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.10.output.LayerNorm.bias as encoder.layer.10.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.11.attention.self.query.weight as encoder.layer.11.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.11.attention.self.query.bias as encoder.layer.11.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.11.attention.self.key.weight as encoder.layer.11.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.11.attention.self.key.bias as encoder.layer.11.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.11.attention.self.value.weight as encoder.layer.11.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.11.attention.self.value.bias as encoder.layer.11.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.11.attention.output.dense.weight as encoder.layer.11.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.11.attention.output.dense.bias as encoder.layer.11.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.11.attention.output.LayerNorm.weight as encoder.layer.11.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.11.attention.output.LayerNorm.bias as encoder.layer.11.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.11.intermediate.dense.weight as encoder.layer.11.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.11.intermediate.dense.bias as encoder.layer.11.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.11.output.dense.weight as encoder.layer.11.output.dense.weight.\n",
      "SAVING bert.encoder.layer.11.output.dense.bias as encoder.layer.11.output.dense.bias.\n",
      "SAVING bert.encoder.layer.11.output.LayerNorm.weight as encoder.layer.11.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.11.output.LayerNorm.bias as encoder.layer.11.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.12.attention.self.query.weight as encoder.layer.12.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.12.attention.self.query.bias as encoder.layer.12.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.12.attention.self.key.weight as encoder.layer.12.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.12.attention.self.key.bias as encoder.layer.12.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.12.attention.self.value.weight as encoder.layer.12.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.12.attention.self.value.bias as encoder.layer.12.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.12.attention.output.dense.weight as encoder.layer.12.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.12.attention.output.dense.bias as encoder.layer.12.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.12.attention.output.LayerNorm.weight as encoder.layer.12.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.12.attention.output.LayerNorm.bias as encoder.layer.12.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.12.intermediate.dense.weight as encoder.layer.12.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.12.intermediate.dense.bias as encoder.layer.12.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.12.output.dense.weight as encoder.layer.12.output.dense.weight.\n",
      "SAVING bert.encoder.layer.12.output.dense.bias as encoder.layer.12.output.dense.bias.\n",
      "SAVING bert.encoder.layer.12.output.LayerNorm.weight as encoder.layer.12.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.12.output.LayerNorm.bias as encoder.layer.12.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.13.attention.self.query.weight as encoder.layer.13.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.13.attention.self.query.bias as encoder.layer.13.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.13.attention.self.key.weight as encoder.layer.13.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.13.attention.self.key.bias as encoder.layer.13.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.13.attention.self.value.weight as encoder.layer.13.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.13.attention.self.value.bias as encoder.layer.13.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.13.attention.output.dense.weight as encoder.layer.13.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.13.attention.output.dense.bias as encoder.layer.13.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.13.attention.output.LayerNorm.weight as encoder.layer.13.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.13.attention.output.LayerNorm.bias as encoder.layer.13.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.13.intermediate.dense.weight as encoder.layer.13.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.13.intermediate.dense.bias as encoder.layer.13.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.13.output.dense.weight as encoder.layer.13.output.dense.weight.\n",
      "SAVING bert.encoder.layer.13.output.dense.bias as encoder.layer.13.output.dense.bias.\n",
      "SAVING bert.encoder.layer.13.output.LayerNorm.weight as encoder.layer.13.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.13.output.LayerNorm.bias as encoder.layer.13.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.14.attention.self.query.weight as encoder.layer.14.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.14.attention.self.query.bias as encoder.layer.14.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.14.attention.self.key.weight as encoder.layer.14.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.14.attention.self.key.bias as encoder.layer.14.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.14.attention.self.value.weight as encoder.layer.14.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.14.attention.self.value.bias as encoder.layer.14.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.14.attention.output.dense.weight as encoder.layer.14.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.14.attention.output.dense.bias as encoder.layer.14.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.14.attention.output.LayerNorm.weight as encoder.layer.14.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.14.attention.output.LayerNorm.bias as encoder.layer.14.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.14.intermediate.dense.weight as encoder.layer.14.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.14.intermediate.dense.bias as encoder.layer.14.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.14.output.dense.weight as encoder.layer.14.output.dense.weight.\n",
      "SAVING bert.encoder.layer.14.output.dense.bias as encoder.layer.14.output.dense.bias.\n",
      "SAVING bert.encoder.layer.14.output.LayerNorm.weight as encoder.layer.14.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.14.output.LayerNorm.bias as encoder.layer.14.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.15.attention.self.query.weight as encoder.layer.15.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.15.attention.self.query.bias as encoder.layer.15.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.15.attention.self.key.weight as encoder.layer.15.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.15.attention.self.key.bias as encoder.layer.15.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.15.attention.self.value.weight as encoder.layer.15.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.15.attention.self.value.bias as encoder.layer.15.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.15.attention.output.dense.weight as encoder.layer.15.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.15.attention.output.dense.bias as encoder.layer.15.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.15.attention.output.LayerNorm.weight as encoder.layer.15.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.15.attention.output.LayerNorm.bias as encoder.layer.15.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.15.intermediate.dense.weight as encoder.layer.15.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.15.intermediate.dense.bias as encoder.layer.15.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.15.output.dense.weight as encoder.layer.15.output.dense.weight.\n",
      "SAVING bert.encoder.layer.15.output.dense.bias as encoder.layer.15.output.dense.bias.\n",
      "SAVING bert.encoder.layer.15.output.LayerNorm.weight as encoder.layer.15.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.15.output.LayerNorm.bias as encoder.layer.15.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.16.attention.self.query.weight as encoder.layer.16.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.16.attention.self.query.bias as encoder.layer.16.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.16.attention.self.key.weight as encoder.layer.16.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.16.attention.self.key.bias as encoder.layer.16.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.16.attention.self.value.weight as encoder.layer.16.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.16.attention.self.value.bias as encoder.layer.16.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.16.attention.output.dense.weight as encoder.layer.16.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.16.attention.output.dense.bias as encoder.layer.16.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.16.attention.output.LayerNorm.weight as encoder.layer.16.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.16.attention.output.LayerNorm.bias as encoder.layer.16.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.16.intermediate.dense.weight as encoder.layer.16.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.16.intermediate.dense.bias as encoder.layer.16.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.16.output.dense.weight as encoder.layer.16.output.dense.weight.\n",
      "SAVING bert.encoder.layer.16.output.dense.bias as encoder.layer.16.output.dense.bias.\n",
      "SAVING bert.encoder.layer.16.output.LayerNorm.weight as encoder.layer.16.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.16.output.LayerNorm.bias as encoder.layer.16.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.17.attention.self.query.weight as encoder.layer.17.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.17.attention.self.query.bias as encoder.layer.17.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.17.attention.self.key.weight as encoder.layer.17.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.17.attention.self.key.bias as encoder.layer.17.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.17.attention.self.value.weight as encoder.layer.17.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.17.attention.self.value.bias as encoder.layer.17.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.17.attention.output.dense.weight as encoder.layer.17.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.17.attention.output.dense.bias as encoder.layer.17.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.17.attention.output.LayerNorm.weight as encoder.layer.17.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.17.attention.output.LayerNorm.bias as encoder.layer.17.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.17.intermediate.dense.weight as encoder.layer.17.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.17.intermediate.dense.bias as encoder.layer.17.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.17.output.dense.weight as encoder.layer.17.output.dense.weight.\n",
      "SAVING bert.encoder.layer.17.output.dense.bias as encoder.layer.17.output.dense.bias.\n",
      "SAVING bert.encoder.layer.17.output.LayerNorm.weight as encoder.layer.17.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.17.output.LayerNorm.bias as encoder.layer.17.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.18.attention.self.query.weight as encoder.layer.18.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.18.attention.self.query.bias as encoder.layer.18.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.18.attention.self.key.weight as encoder.layer.18.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.18.attention.self.key.bias as encoder.layer.18.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.18.attention.self.value.weight as encoder.layer.18.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.18.attention.self.value.bias as encoder.layer.18.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.18.attention.output.dense.weight as encoder.layer.18.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.18.attention.output.dense.bias as encoder.layer.18.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.18.attention.output.LayerNorm.weight as encoder.layer.18.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.18.attention.output.LayerNorm.bias as encoder.layer.18.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.18.intermediate.dense.weight as encoder.layer.18.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.18.intermediate.dense.bias as encoder.layer.18.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.18.output.dense.weight as encoder.layer.18.output.dense.weight.\n",
      "SAVING bert.encoder.layer.18.output.dense.bias as encoder.layer.18.output.dense.bias.\n",
      "SAVING bert.encoder.layer.18.output.LayerNorm.weight as encoder.layer.18.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.18.output.LayerNorm.bias as encoder.layer.18.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.19.attention.self.query.weight as encoder.layer.19.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.19.attention.self.query.bias as encoder.layer.19.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.19.attention.self.key.weight as encoder.layer.19.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.19.attention.self.key.bias as encoder.layer.19.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.19.attention.self.value.weight as encoder.layer.19.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.19.attention.self.value.bias as encoder.layer.19.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.19.attention.output.dense.weight as encoder.layer.19.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.19.attention.output.dense.bias as encoder.layer.19.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.19.attention.output.LayerNorm.weight as encoder.layer.19.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.19.attention.output.LayerNorm.bias as encoder.layer.19.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.19.intermediate.dense.weight as encoder.layer.19.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.19.intermediate.dense.bias as encoder.layer.19.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.19.output.dense.weight as encoder.layer.19.output.dense.weight.\n",
      "SAVING bert.encoder.layer.19.output.dense.bias as encoder.layer.19.output.dense.bias.\n",
      "SAVING bert.encoder.layer.19.output.LayerNorm.weight as encoder.layer.19.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.19.output.LayerNorm.bias as encoder.layer.19.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.20.attention.self.query.weight as encoder.layer.20.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.20.attention.self.query.bias as encoder.layer.20.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.20.attention.self.key.weight as encoder.layer.20.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.20.attention.self.key.bias as encoder.layer.20.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.20.attention.self.value.weight as encoder.layer.20.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.20.attention.self.value.bias as encoder.layer.20.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.20.attention.output.dense.weight as encoder.layer.20.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.20.attention.output.dense.bias as encoder.layer.20.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.20.attention.output.LayerNorm.weight as encoder.layer.20.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.20.attention.output.LayerNorm.bias as encoder.layer.20.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.20.intermediate.dense.weight as encoder.layer.20.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.20.intermediate.dense.bias as encoder.layer.20.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.20.output.dense.weight as encoder.layer.20.output.dense.weight.\n",
      "SAVING bert.encoder.layer.20.output.dense.bias as encoder.layer.20.output.dense.bias.\n",
      "SAVING bert.encoder.layer.20.output.LayerNorm.weight as encoder.layer.20.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.20.output.LayerNorm.bias as encoder.layer.20.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.21.attention.self.query.weight as encoder.layer.21.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.21.attention.self.query.bias as encoder.layer.21.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.21.attention.self.key.weight as encoder.layer.21.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.21.attention.self.key.bias as encoder.layer.21.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.21.attention.self.value.weight as encoder.layer.21.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.21.attention.self.value.bias as encoder.layer.21.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.21.attention.output.dense.weight as encoder.layer.21.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.21.attention.output.dense.bias as encoder.layer.21.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.21.attention.output.LayerNorm.weight as encoder.layer.21.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.21.attention.output.LayerNorm.bias as encoder.layer.21.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.21.intermediate.dense.weight as encoder.layer.21.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.21.intermediate.dense.bias as encoder.layer.21.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.21.output.dense.weight as encoder.layer.21.output.dense.weight.\n",
      "SAVING bert.encoder.layer.21.output.dense.bias as encoder.layer.21.output.dense.bias.\n",
      "SAVING bert.encoder.layer.21.output.LayerNorm.weight as encoder.layer.21.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.21.output.LayerNorm.bias as encoder.layer.21.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.22.attention.self.query.weight as encoder.layer.22.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.22.attention.self.query.bias as encoder.layer.22.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.22.attention.self.key.weight as encoder.layer.22.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.22.attention.self.key.bias as encoder.layer.22.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.22.attention.self.value.weight as encoder.layer.22.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.22.attention.self.value.bias as encoder.layer.22.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.22.attention.output.dense.weight as encoder.layer.22.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.22.attention.output.dense.bias as encoder.layer.22.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.22.attention.output.LayerNorm.weight as encoder.layer.22.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.22.attention.output.LayerNorm.bias as encoder.layer.22.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.22.intermediate.dense.weight as encoder.layer.22.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.22.intermediate.dense.bias as encoder.layer.22.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.22.output.dense.weight as encoder.layer.22.output.dense.weight.\n",
      "SAVING bert.encoder.layer.22.output.dense.bias as encoder.layer.22.output.dense.bias.\n",
      "SAVING bert.encoder.layer.22.output.LayerNorm.weight as encoder.layer.22.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.22.output.LayerNorm.bias as encoder.layer.22.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.23.attention.self.query.weight as encoder.layer.23.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.23.attention.self.query.bias as encoder.layer.23.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.23.attention.self.key.weight as encoder.layer.23.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.23.attention.self.key.bias as encoder.layer.23.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.23.attention.self.value.weight as encoder.layer.23.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.23.attention.self.value.bias as encoder.layer.23.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.23.attention.output.dense.weight as encoder.layer.23.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.23.attention.output.dense.bias as encoder.layer.23.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.23.attention.output.LayerNorm.weight as encoder.layer.23.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.23.attention.output.LayerNorm.bias as encoder.layer.23.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.23.intermediate.dense.weight as encoder.layer.23.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.23.intermediate.dense.bias as encoder.layer.23.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.23.output.dense.weight as encoder.layer.23.output.dense.weight.\n",
      "SAVING bert.encoder.layer.23.output.dense.bias as encoder.layer.23.output.dense.bias.\n",
      "SAVING bert.encoder.layer.23.output.LayerNorm.weight as encoder.layer.23.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.23.output.LayerNorm.bias as encoder.layer.23.output.LayerNorm.bias.\n",
      "SAVING bert.pooler.dense.weight as pooler.dense.weight.\n",
      "SAVING bert.pooler.dense.bias as pooler.dense.bias.\n",
      "MODIFYING: bert.img_embedding.weight\n",
      "SAVING bert.img_embedding.bias as img_embedding.bias.\n",
      "\n",
      "Weights in loaded but not in model:\n",
      "cls.predictions.bias\n",
      "cls.predictions.decoder.weight\n",
      "cls.predictions.transform.LayerNorm.bias\n",
      "cls.predictions.transform.LayerNorm.weight\n",
      "cls.predictions.transform.dense.bias\n",
      "cls.predictions.transform.dense.weight\n",
      "cls.seq_relationship.bias\n",
      "cls.seq_relationship.weight\n",
      "\n",
      "Weights in model but not in loaded:\n",
      "\n",
      "Total Iters: 4690\n",
      "/opt/conda/lib/python3.7/site-packages/torchcontrib/optim/swa.py:130: UserWarning: Casting swa_start, swa_freq to int\n",
      "  warnings.warn(\"Casting swa_start, swa_freq to int\")\n",
      "Splits in Train data: ['train']\n",
      "Splits in Valid data: ['dev_seen']\n",
      "Batches: 938\n",
      "tensor([-1.1208, -0.3946], device='cuda:0')\n",
      "\n",
      "Epoch(U) 0(250): Train AC 61.20 RA 55.0371 LOSS 1354.1791\n",
      "\n",
      "Epoch(U) 0(250): DEV AC 52.40 RA 53.2880 \n",
      "Epoch(U) 0(250): BEST AC 52.40 RA 53.2880 \n",
      "\n",
      "Epoch(U) 0(500): Train AC 62.85 RA 59.8452 LOSS 1303.1201\n",
      "\n",
      "Epoch(U) 0(500): DEV AC 55.80 RA 64.3472 \n",
      "Epoch(U) 0(500): BEST AC 55.80 RA 64.3472 \n",
      "\n",
      "Epoch(U) 0(750): Train AC 65.05 RA 64.4935 LOSS 1220.3578\n",
      "\n",
      "Epoch(U) 0(750): DEV AC 59.40 RA 67.7280 \n",
      "Epoch(U) 0(750): BEST AC 59.40 RA 67.7280 \n",
      "tensor([-0.2034, -1.6927], device='cuda:0')\n",
      "\n",
      "Epoch(U) 1(1000): Train AC 76.41 RA 82.9188 LOSS 1076.7648\n",
      "\n",
      "Epoch(U) 1(1000): DEV AC 59.40 RA 69.5744 \n",
      "Epoch(U) 1(1000): BEST AC 59.40 RA 69.5744 \n",
      "\n",
      "Epoch(U) 1(1250): Train AC 76.08 RA 82.1308 LOSS 1026.2499\n",
      "\n",
      "Epoch(U) 1(1250): DEV AC 62.40 RA 72.1136 \n",
      "Epoch(U) 1(1250): BEST AC 62.40 RA 72.1136 \n",
      "\n",
      "Epoch(U) 1(1500): Train AC 76.56 RA 82.6069 LOSS 994.1375\n",
      "\n",
      "Epoch(U) 1(1500): DEV AC 66.20 RA 73.0208 \n",
      "Epoch(U) 1(1500): BEST AC 66.20 RA 73.0208 \n",
      "\n",
      "Epoch(U) 1(1750): Train AC 76.86 RA 82.7794 LOSS 1005.9540\n",
      "\n",
      "Epoch(U) 1(1750): DEV AC 65.00 RA 73.3376 \n",
      "Epoch(U) 1(1750): BEST AC 65.00 RA 73.3376 \n",
      "tensor([-0.1260, -2.1335], device='cuda:0')\n",
      "\n",
      "Epoch(U) 2(2000): Train AC 81.75 RA 90.1637 LOSS 875.4559\n",
      "\n",
      "Epoch(U) 2(2000): DEV AC 60.80 RA 74.5616 \n",
      "Epoch(U) 2(2000): BEST AC 60.80 RA 74.5616 \n",
      "\n",
      "Epoch(U) 2(2250): Train AC 82.52 RA 89.4011 LOSS 827.4856\n",
      "\n",
      "Epoch(U) 2(2250): DEV AC 61.20 RA 75.9584 \n",
      "Epoch(U) 2(2250): BEST AC 61.20 RA 75.9584 \n",
      "\n",
      "Epoch(U) 2(2500): Train AC 83.13 RA 89.7562 LOSS 769.0107\n",
      "\n",
      "Epoch(U) 2(2500): DEV AC 62.60 RA 73.9760 \n",
      "Epoch(U) 2(2500): BEST AC 61.20 RA 75.9584 \n",
      "\n",
      "Epoch(U) 2(2750): Train AC 83.24 RA 90.0801 LOSS 787.3708\n",
      "\n",
      "Epoch(U) 2(2750): DEV AC 64.80 RA 75.0288 \n",
      "Epoch(U) 2(2750): BEST AC 61.20 RA 75.9584 \n",
      "tensor([-0.0346, -3.3824], device='cuda:0')\n",
      "\n",
      "Epoch(U) 3(3250): Train AC 87.64 RA 93.7415 LOSS 719.7939\n",
      "\n",
      "Epoch(U) 3(3250): DEV AC 66.60 RA 77.1344 \n",
      "Epoch(U) 3(3250): BEST AC 66.60 RA 77.1344 \n",
      "\n",
      "Epoch(U) 3(3500): Train AC 87.86 RA 93.9610 LOSS 645.9868\n",
      "\n",
      "Epoch(U) 3(3500): DEV AC 64.80 RA 76.2096 \n",
      "Epoch(U) 3(3500): BEST AC 66.60 RA 77.1344 \n",
      "\n",
      "Epoch(U) 3(3750): Train AC 87.69 RA 93.8068 LOSS 721.0873\n",
      "\n",
      "Epoch(U) 3(3750): DEV AC 62.80 RA 76.4544 \n",
      "Epoch(U) 3(3750): BEST AC 66.60 RA 77.1344 \n",
      "tensor([-6.5784e+00, -1.3910e-03], device='cuda:0')\n",
      "\n",
      "Epoch(U) 4(4000): Train AC 87.55 RA 94.0369 LOSS 720.9170\n",
      "\n",
      "Epoch(U) 4(4000): DEV AC 66.20 RA 76.7424 \n",
      "Epoch(U) 4(4000): BEST AC 66.60 RA 77.1344 \n",
      "\n",
      "Epoch(U) 4(4250): Train AC 87.00 RA 93.8739 LOSS 719.7699\n",
      "\n",
      "Epoch(U) 4(4250): DEV AC 71.20 RA 77.5312 \n",
      "Epoch(U) 4(4250): BEST AC 71.20 RA 77.5312 \n",
      "\n",
      "Epoch(U) 4(4500): Train AC 87.42 RA 93.7490 LOSS 757.4856\n",
      "\n",
      "Epoch(U) 4(4500): DEV AC 60.20 RA 75.1248 \n",
      "Epoch(U) 4(4500): BEST AC 71.20 RA 77.5312 \n",
      "Load model from ./data/LASTtrain.pth\n",
      "Load 500 data from split(s) dev_seen.\n",
      "Start to load Faster-RCNN detected objects from data/HM_img.tsv\n",
      "Loaded 500 images in file data/HM_img.tsv in 34 seconds.\n",
      "Use 500 data in torch dataset\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   id      500 non-null    int64  \n",
      " 1   proba   500 non-null    float64\n",
      " 2   label   500 non-null    int64  \n",
      "dtypes: float64(1), int64(2)\n",
      "memory usage: 11.8 KB\n",
      "None\n",
      "(0.656, 0.7725599999999999)\n",
      "2022-05-06 22:53:39.069090: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-05-06 22:53:39.069134: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "Load 8959 data from split(s) traindev.\n",
      "Start to load Faster-RCNN detected objects from data/HM_img.tsv\n",
      "Loaded 7500 images in file data/HM_img.tsv in 40 seconds.\n",
      "Use 7500 data in torch dataset\n",
      "\n",
      "Load 500 data from split(s) dev_seen.\n",
      "Start to load Faster-RCNN detected objects from data/HM_img.tsv\n",
      "Loaded 500 images in file data/HM_img.tsv in 34 seconds.\n",
      "Use 500 data in torch dataset\n",
      "\n",
      "Some weights of BertO were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['bert.img_embedding.weight', 'bert.img_embedding.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "UNEXPECTED:  []\n",
      "MISSING:  ['bert.img_embedding.weight', 'bert.img_embedding.bias']\n",
      "ERRORS:  []\n",
      "REINITING:  Linear(in_features=1024, out_features=2048, bias=True)\n",
      "REINITING:  GeLU()\n",
      "REINITING:  LayerNorm((2048,), eps=1e-12, elementwise_affine=True)\n",
      "REINITING:  Linear(in_features=2048, out_features=2, bias=True)\n",
      "REINITING:  Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (1): GeLU()\n",
      "  (2): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)\n",
      "  (3): Linear(in_features=2048, out_features=2, bias=True)\n",
      ")\n",
      "Load pre-trained model from ./data/LAST_BO.pth\n",
      "SAVING bert.embeddings.position_ids as embeddings.position_ids.\n",
      "SAVING bert.embeddings.word_embeddings.weight as embeddings.word_embeddings.weight.\n",
      "SAVING bert.embeddings.position_embeddings.weight as embeddings.position_embeddings.weight.\n",
      "SAVING bert.embeddings.token_type_embeddings.weight as embeddings.token_type_embeddings.weight.\n",
      "SAVING bert.embeddings.LayerNorm.weight as embeddings.LayerNorm.weight.\n",
      "SAVING bert.embeddings.LayerNorm.bias as embeddings.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.0.attention.self.query.weight as encoder.layer.0.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.0.attention.self.query.bias as encoder.layer.0.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.0.attention.self.key.weight as encoder.layer.0.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.0.attention.self.key.bias as encoder.layer.0.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.0.attention.self.value.weight as encoder.layer.0.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.0.attention.self.value.bias as encoder.layer.0.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.0.attention.output.dense.weight as encoder.layer.0.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.0.attention.output.dense.bias as encoder.layer.0.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.0.attention.output.LayerNorm.weight as encoder.layer.0.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.0.attention.output.LayerNorm.bias as encoder.layer.0.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.0.intermediate.dense.weight as encoder.layer.0.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.0.intermediate.dense.bias as encoder.layer.0.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.0.output.dense.weight as encoder.layer.0.output.dense.weight.\n",
      "SAVING bert.encoder.layer.0.output.dense.bias as encoder.layer.0.output.dense.bias.\n",
      "SAVING bert.encoder.layer.0.output.LayerNorm.weight as encoder.layer.0.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.0.output.LayerNorm.bias as encoder.layer.0.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.1.attention.self.query.weight as encoder.layer.1.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.1.attention.self.query.bias as encoder.layer.1.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.1.attention.self.key.weight as encoder.layer.1.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.1.attention.self.key.bias as encoder.layer.1.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.1.attention.self.value.weight as encoder.layer.1.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.1.attention.self.value.bias as encoder.layer.1.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.1.attention.output.dense.weight as encoder.layer.1.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.1.attention.output.dense.bias as encoder.layer.1.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.1.attention.output.LayerNorm.weight as encoder.layer.1.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.1.attention.output.LayerNorm.bias as encoder.layer.1.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.1.intermediate.dense.weight as encoder.layer.1.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.1.intermediate.dense.bias as encoder.layer.1.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.1.output.dense.weight as encoder.layer.1.output.dense.weight.\n",
      "SAVING bert.encoder.layer.1.output.dense.bias as encoder.layer.1.output.dense.bias.\n",
      "SAVING bert.encoder.layer.1.output.LayerNorm.weight as encoder.layer.1.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.1.output.LayerNorm.bias as encoder.layer.1.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.2.attention.self.query.weight as encoder.layer.2.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.2.attention.self.query.bias as encoder.layer.2.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.2.attention.self.key.weight as encoder.layer.2.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.2.attention.self.key.bias as encoder.layer.2.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.2.attention.self.value.weight as encoder.layer.2.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.2.attention.self.value.bias as encoder.layer.2.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.2.attention.output.dense.weight as encoder.layer.2.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.2.attention.output.dense.bias as encoder.layer.2.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.2.attention.output.LayerNorm.weight as encoder.layer.2.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.2.attention.output.LayerNorm.bias as encoder.layer.2.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.2.intermediate.dense.weight as encoder.layer.2.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.2.intermediate.dense.bias as encoder.layer.2.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.2.output.dense.weight as encoder.layer.2.output.dense.weight.\n",
      "SAVING bert.encoder.layer.2.output.dense.bias as encoder.layer.2.output.dense.bias.\n",
      "SAVING bert.encoder.layer.2.output.LayerNorm.weight as encoder.layer.2.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.2.output.LayerNorm.bias as encoder.layer.2.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.3.attention.self.query.weight as encoder.layer.3.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.3.attention.self.query.bias as encoder.layer.3.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.3.attention.self.key.weight as encoder.layer.3.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.3.attention.self.key.bias as encoder.layer.3.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.3.attention.self.value.weight as encoder.layer.3.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.3.attention.self.value.bias as encoder.layer.3.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.3.attention.output.dense.weight as encoder.layer.3.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.3.attention.output.dense.bias as encoder.layer.3.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.3.attention.output.LayerNorm.weight as encoder.layer.3.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.3.attention.output.LayerNorm.bias as encoder.layer.3.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.3.intermediate.dense.weight as encoder.layer.3.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.3.intermediate.dense.bias as encoder.layer.3.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.3.output.dense.weight as encoder.layer.3.output.dense.weight.\n",
      "SAVING bert.encoder.layer.3.output.dense.bias as encoder.layer.3.output.dense.bias.\n",
      "SAVING bert.encoder.layer.3.output.LayerNorm.weight as encoder.layer.3.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.3.output.LayerNorm.bias as encoder.layer.3.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.4.attention.self.query.weight as encoder.layer.4.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.4.attention.self.query.bias as encoder.layer.4.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.4.attention.self.key.weight as encoder.layer.4.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.4.attention.self.key.bias as encoder.layer.4.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.4.attention.self.value.weight as encoder.layer.4.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.4.attention.self.value.bias as encoder.layer.4.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.4.attention.output.dense.weight as encoder.layer.4.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.4.attention.output.dense.bias as encoder.layer.4.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.4.attention.output.LayerNorm.weight as encoder.layer.4.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.4.attention.output.LayerNorm.bias as encoder.layer.4.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.4.intermediate.dense.weight as encoder.layer.4.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.4.intermediate.dense.bias as encoder.layer.4.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.4.output.dense.weight as encoder.layer.4.output.dense.weight.\n",
      "SAVING bert.encoder.layer.4.output.dense.bias as encoder.layer.4.output.dense.bias.\n",
      "SAVING bert.encoder.layer.4.output.LayerNorm.weight as encoder.layer.4.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.4.output.LayerNorm.bias as encoder.layer.4.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.5.attention.self.query.weight as encoder.layer.5.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.5.attention.self.query.bias as encoder.layer.5.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.5.attention.self.key.weight as encoder.layer.5.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.5.attention.self.key.bias as encoder.layer.5.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.5.attention.self.value.weight as encoder.layer.5.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.5.attention.self.value.bias as encoder.layer.5.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.5.attention.output.dense.weight as encoder.layer.5.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.5.attention.output.dense.bias as encoder.layer.5.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.5.attention.output.LayerNorm.weight as encoder.layer.5.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.5.attention.output.LayerNorm.bias as encoder.layer.5.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.5.intermediate.dense.weight as encoder.layer.5.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.5.intermediate.dense.bias as encoder.layer.5.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.5.output.dense.weight as encoder.layer.5.output.dense.weight.\n",
      "SAVING bert.encoder.layer.5.output.dense.bias as encoder.layer.5.output.dense.bias.\n",
      "SAVING bert.encoder.layer.5.output.LayerNorm.weight as encoder.layer.5.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.5.output.LayerNorm.bias as encoder.layer.5.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.6.attention.self.query.weight as encoder.layer.6.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.6.attention.self.query.bias as encoder.layer.6.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.6.attention.self.key.weight as encoder.layer.6.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.6.attention.self.key.bias as encoder.layer.6.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.6.attention.self.value.weight as encoder.layer.6.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.6.attention.self.value.bias as encoder.layer.6.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.6.attention.output.dense.weight as encoder.layer.6.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.6.attention.output.dense.bias as encoder.layer.6.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.6.attention.output.LayerNorm.weight as encoder.layer.6.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.6.attention.output.LayerNorm.bias as encoder.layer.6.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.6.intermediate.dense.weight as encoder.layer.6.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.6.intermediate.dense.bias as encoder.layer.6.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.6.output.dense.weight as encoder.layer.6.output.dense.weight.\n",
      "SAVING bert.encoder.layer.6.output.dense.bias as encoder.layer.6.output.dense.bias.\n",
      "SAVING bert.encoder.layer.6.output.LayerNorm.weight as encoder.layer.6.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.6.output.LayerNorm.bias as encoder.layer.6.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.7.attention.self.query.weight as encoder.layer.7.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.7.attention.self.query.bias as encoder.layer.7.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.7.attention.self.key.weight as encoder.layer.7.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.7.attention.self.key.bias as encoder.layer.7.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.7.attention.self.value.weight as encoder.layer.7.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.7.attention.self.value.bias as encoder.layer.7.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.7.attention.output.dense.weight as encoder.layer.7.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.7.attention.output.dense.bias as encoder.layer.7.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.7.attention.output.LayerNorm.weight as encoder.layer.7.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.7.attention.output.LayerNorm.bias as encoder.layer.7.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.7.intermediate.dense.weight as encoder.layer.7.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.7.intermediate.dense.bias as encoder.layer.7.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.7.output.dense.weight as encoder.layer.7.output.dense.weight.\n",
      "SAVING bert.encoder.layer.7.output.dense.bias as encoder.layer.7.output.dense.bias.\n",
      "SAVING bert.encoder.layer.7.output.LayerNorm.weight as encoder.layer.7.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.7.output.LayerNorm.bias as encoder.layer.7.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.8.attention.self.query.weight as encoder.layer.8.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.8.attention.self.query.bias as encoder.layer.8.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.8.attention.self.key.weight as encoder.layer.8.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.8.attention.self.key.bias as encoder.layer.8.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.8.attention.self.value.weight as encoder.layer.8.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.8.attention.self.value.bias as encoder.layer.8.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.8.attention.output.dense.weight as encoder.layer.8.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.8.attention.output.dense.bias as encoder.layer.8.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.8.attention.output.LayerNorm.weight as encoder.layer.8.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.8.attention.output.LayerNorm.bias as encoder.layer.8.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.8.intermediate.dense.weight as encoder.layer.8.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.8.intermediate.dense.bias as encoder.layer.8.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.8.output.dense.weight as encoder.layer.8.output.dense.weight.\n",
      "SAVING bert.encoder.layer.8.output.dense.bias as encoder.layer.8.output.dense.bias.\n",
      "SAVING bert.encoder.layer.8.output.LayerNorm.weight as encoder.layer.8.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.8.output.LayerNorm.bias as encoder.layer.8.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.9.attention.self.query.weight as encoder.layer.9.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.9.attention.self.query.bias as encoder.layer.9.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.9.attention.self.key.weight as encoder.layer.9.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.9.attention.self.key.bias as encoder.layer.9.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.9.attention.self.value.weight as encoder.layer.9.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.9.attention.self.value.bias as encoder.layer.9.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.9.attention.output.dense.weight as encoder.layer.9.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.9.attention.output.dense.bias as encoder.layer.9.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.9.attention.output.LayerNorm.weight as encoder.layer.9.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.9.attention.output.LayerNorm.bias as encoder.layer.9.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.9.intermediate.dense.weight as encoder.layer.9.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.9.intermediate.dense.bias as encoder.layer.9.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.9.output.dense.weight as encoder.layer.9.output.dense.weight.\n",
      "SAVING bert.encoder.layer.9.output.dense.bias as encoder.layer.9.output.dense.bias.\n",
      "SAVING bert.encoder.layer.9.output.LayerNorm.weight as encoder.layer.9.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.9.output.LayerNorm.bias as encoder.layer.9.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.10.attention.self.query.weight as encoder.layer.10.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.10.attention.self.query.bias as encoder.layer.10.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.10.attention.self.key.weight as encoder.layer.10.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.10.attention.self.key.bias as encoder.layer.10.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.10.attention.self.value.weight as encoder.layer.10.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.10.attention.self.value.bias as encoder.layer.10.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.10.attention.output.dense.weight as encoder.layer.10.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.10.attention.output.dense.bias as encoder.layer.10.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.10.attention.output.LayerNorm.weight as encoder.layer.10.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.10.attention.output.LayerNorm.bias as encoder.layer.10.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.10.intermediate.dense.weight as encoder.layer.10.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.10.intermediate.dense.bias as encoder.layer.10.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.10.output.dense.weight as encoder.layer.10.output.dense.weight.\n",
      "SAVING bert.encoder.layer.10.output.dense.bias as encoder.layer.10.output.dense.bias.\n",
      "SAVING bert.encoder.layer.10.output.LayerNorm.weight as encoder.layer.10.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.10.output.LayerNorm.bias as encoder.layer.10.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.11.attention.self.query.weight as encoder.layer.11.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.11.attention.self.query.bias as encoder.layer.11.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.11.attention.self.key.weight as encoder.layer.11.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.11.attention.self.key.bias as encoder.layer.11.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.11.attention.self.value.weight as encoder.layer.11.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.11.attention.self.value.bias as encoder.layer.11.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.11.attention.output.dense.weight as encoder.layer.11.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.11.attention.output.dense.bias as encoder.layer.11.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.11.attention.output.LayerNorm.weight as encoder.layer.11.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.11.attention.output.LayerNorm.bias as encoder.layer.11.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.11.intermediate.dense.weight as encoder.layer.11.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.11.intermediate.dense.bias as encoder.layer.11.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.11.output.dense.weight as encoder.layer.11.output.dense.weight.\n",
      "SAVING bert.encoder.layer.11.output.dense.bias as encoder.layer.11.output.dense.bias.\n",
      "SAVING bert.encoder.layer.11.output.LayerNorm.weight as encoder.layer.11.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.11.output.LayerNorm.bias as encoder.layer.11.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.12.attention.self.query.weight as encoder.layer.12.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.12.attention.self.query.bias as encoder.layer.12.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.12.attention.self.key.weight as encoder.layer.12.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.12.attention.self.key.bias as encoder.layer.12.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.12.attention.self.value.weight as encoder.layer.12.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.12.attention.self.value.bias as encoder.layer.12.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.12.attention.output.dense.weight as encoder.layer.12.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.12.attention.output.dense.bias as encoder.layer.12.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.12.attention.output.LayerNorm.weight as encoder.layer.12.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.12.attention.output.LayerNorm.bias as encoder.layer.12.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.12.intermediate.dense.weight as encoder.layer.12.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.12.intermediate.dense.bias as encoder.layer.12.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.12.output.dense.weight as encoder.layer.12.output.dense.weight.\n",
      "SAVING bert.encoder.layer.12.output.dense.bias as encoder.layer.12.output.dense.bias.\n",
      "SAVING bert.encoder.layer.12.output.LayerNorm.weight as encoder.layer.12.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.12.output.LayerNorm.bias as encoder.layer.12.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.13.attention.self.query.weight as encoder.layer.13.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.13.attention.self.query.bias as encoder.layer.13.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.13.attention.self.key.weight as encoder.layer.13.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.13.attention.self.key.bias as encoder.layer.13.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.13.attention.self.value.weight as encoder.layer.13.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.13.attention.self.value.bias as encoder.layer.13.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.13.attention.output.dense.weight as encoder.layer.13.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.13.attention.output.dense.bias as encoder.layer.13.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.13.attention.output.LayerNorm.weight as encoder.layer.13.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.13.attention.output.LayerNorm.bias as encoder.layer.13.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.13.intermediate.dense.weight as encoder.layer.13.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.13.intermediate.dense.bias as encoder.layer.13.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.13.output.dense.weight as encoder.layer.13.output.dense.weight.\n",
      "SAVING bert.encoder.layer.13.output.dense.bias as encoder.layer.13.output.dense.bias.\n",
      "SAVING bert.encoder.layer.13.output.LayerNorm.weight as encoder.layer.13.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.13.output.LayerNorm.bias as encoder.layer.13.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.14.attention.self.query.weight as encoder.layer.14.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.14.attention.self.query.bias as encoder.layer.14.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.14.attention.self.key.weight as encoder.layer.14.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.14.attention.self.key.bias as encoder.layer.14.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.14.attention.self.value.weight as encoder.layer.14.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.14.attention.self.value.bias as encoder.layer.14.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.14.attention.output.dense.weight as encoder.layer.14.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.14.attention.output.dense.bias as encoder.layer.14.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.14.attention.output.LayerNorm.weight as encoder.layer.14.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.14.attention.output.LayerNorm.bias as encoder.layer.14.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.14.intermediate.dense.weight as encoder.layer.14.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.14.intermediate.dense.bias as encoder.layer.14.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.14.output.dense.weight as encoder.layer.14.output.dense.weight.\n",
      "SAVING bert.encoder.layer.14.output.dense.bias as encoder.layer.14.output.dense.bias.\n",
      "SAVING bert.encoder.layer.14.output.LayerNorm.weight as encoder.layer.14.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.14.output.LayerNorm.bias as encoder.layer.14.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.15.attention.self.query.weight as encoder.layer.15.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.15.attention.self.query.bias as encoder.layer.15.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.15.attention.self.key.weight as encoder.layer.15.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.15.attention.self.key.bias as encoder.layer.15.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.15.attention.self.value.weight as encoder.layer.15.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.15.attention.self.value.bias as encoder.layer.15.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.15.attention.output.dense.weight as encoder.layer.15.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.15.attention.output.dense.bias as encoder.layer.15.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.15.attention.output.LayerNorm.weight as encoder.layer.15.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.15.attention.output.LayerNorm.bias as encoder.layer.15.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.15.intermediate.dense.weight as encoder.layer.15.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.15.intermediate.dense.bias as encoder.layer.15.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.15.output.dense.weight as encoder.layer.15.output.dense.weight.\n",
      "SAVING bert.encoder.layer.15.output.dense.bias as encoder.layer.15.output.dense.bias.\n",
      "SAVING bert.encoder.layer.15.output.LayerNorm.weight as encoder.layer.15.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.15.output.LayerNorm.bias as encoder.layer.15.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.16.attention.self.query.weight as encoder.layer.16.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.16.attention.self.query.bias as encoder.layer.16.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.16.attention.self.key.weight as encoder.layer.16.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.16.attention.self.key.bias as encoder.layer.16.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.16.attention.self.value.weight as encoder.layer.16.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.16.attention.self.value.bias as encoder.layer.16.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.16.attention.output.dense.weight as encoder.layer.16.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.16.attention.output.dense.bias as encoder.layer.16.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.16.attention.output.LayerNorm.weight as encoder.layer.16.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.16.attention.output.LayerNorm.bias as encoder.layer.16.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.16.intermediate.dense.weight as encoder.layer.16.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.16.intermediate.dense.bias as encoder.layer.16.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.16.output.dense.weight as encoder.layer.16.output.dense.weight.\n",
      "SAVING bert.encoder.layer.16.output.dense.bias as encoder.layer.16.output.dense.bias.\n",
      "SAVING bert.encoder.layer.16.output.LayerNorm.weight as encoder.layer.16.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.16.output.LayerNorm.bias as encoder.layer.16.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.17.attention.self.query.weight as encoder.layer.17.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.17.attention.self.query.bias as encoder.layer.17.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.17.attention.self.key.weight as encoder.layer.17.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.17.attention.self.key.bias as encoder.layer.17.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.17.attention.self.value.weight as encoder.layer.17.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.17.attention.self.value.bias as encoder.layer.17.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.17.attention.output.dense.weight as encoder.layer.17.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.17.attention.output.dense.bias as encoder.layer.17.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.17.attention.output.LayerNorm.weight as encoder.layer.17.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.17.attention.output.LayerNorm.bias as encoder.layer.17.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.17.intermediate.dense.weight as encoder.layer.17.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.17.intermediate.dense.bias as encoder.layer.17.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.17.output.dense.weight as encoder.layer.17.output.dense.weight.\n",
      "SAVING bert.encoder.layer.17.output.dense.bias as encoder.layer.17.output.dense.bias.\n",
      "SAVING bert.encoder.layer.17.output.LayerNorm.weight as encoder.layer.17.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.17.output.LayerNorm.bias as encoder.layer.17.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.18.attention.self.query.weight as encoder.layer.18.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.18.attention.self.query.bias as encoder.layer.18.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.18.attention.self.key.weight as encoder.layer.18.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.18.attention.self.key.bias as encoder.layer.18.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.18.attention.self.value.weight as encoder.layer.18.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.18.attention.self.value.bias as encoder.layer.18.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.18.attention.output.dense.weight as encoder.layer.18.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.18.attention.output.dense.bias as encoder.layer.18.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.18.attention.output.LayerNorm.weight as encoder.layer.18.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.18.attention.output.LayerNorm.bias as encoder.layer.18.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.18.intermediate.dense.weight as encoder.layer.18.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.18.intermediate.dense.bias as encoder.layer.18.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.18.output.dense.weight as encoder.layer.18.output.dense.weight.\n",
      "SAVING bert.encoder.layer.18.output.dense.bias as encoder.layer.18.output.dense.bias.\n",
      "SAVING bert.encoder.layer.18.output.LayerNorm.weight as encoder.layer.18.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.18.output.LayerNorm.bias as encoder.layer.18.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.19.attention.self.query.weight as encoder.layer.19.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.19.attention.self.query.bias as encoder.layer.19.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.19.attention.self.key.weight as encoder.layer.19.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.19.attention.self.key.bias as encoder.layer.19.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.19.attention.self.value.weight as encoder.layer.19.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.19.attention.self.value.bias as encoder.layer.19.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.19.attention.output.dense.weight as encoder.layer.19.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.19.attention.output.dense.bias as encoder.layer.19.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.19.attention.output.LayerNorm.weight as encoder.layer.19.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.19.attention.output.LayerNorm.bias as encoder.layer.19.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.19.intermediate.dense.weight as encoder.layer.19.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.19.intermediate.dense.bias as encoder.layer.19.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.19.output.dense.weight as encoder.layer.19.output.dense.weight.\n",
      "SAVING bert.encoder.layer.19.output.dense.bias as encoder.layer.19.output.dense.bias.\n",
      "SAVING bert.encoder.layer.19.output.LayerNorm.weight as encoder.layer.19.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.19.output.LayerNorm.bias as encoder.layer.19.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.20.attention.self.query.weight as encoder.layer.20.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.20.attention.self.query.bias as encoder.layer.20.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.20.attention.self.key.weight as encoder.layer.20.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.20.attention.self.key.bias as encoder.layer.20.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.20.attention.self.value.weight as encoder.layer.20.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.20.attention.self.value.bias as encoder.layer.20.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.20.attention.output.dense.weight as encoder.layer.20.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.20.attention.output.dense.bias as encoder.layer.20.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.20.attention.output.LayerNorm.weight as encoder.layer.20.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.20.attention.output.LayerNorm.bias as encoder.layer.20.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.20.intermediate.dense.weight as encoder.layer.20.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.20.intermediate.dense.bias as encoder.layer.20.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.20.output.dense.weight as encoder.layer.20.output.dense.weight.\n",
      "SAVING bert.encoder.layer.20.output.dense.bias as encoder.layer.20.output.dense.bias.\n",
      "SAVING bert.encoder.layer.20.output.LayerNorm.weight as encoder.layer.20.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.20.output.LayerNorm.bias as encoder.layer.20.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.21.attention.self.query.weight as encoder.layer.21.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.21.attention.self.query.bias as encoder.layer.21.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.21.attention.self.key.weight as encoder.layer.21.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.21.attention.self.key.bias as encoder.layer.21.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.21.attention.self.value.weight as encoder.layer.21.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.21.attention.self.value.bias as encoder.layer.21.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.21.attention.output.dense.weight as encoder.layer.21.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.21.attention.output.dense.bias as encoder.layer.21.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.21.attention.output.LayerNorm.weight as encoder.layer.21.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.21.attention.output.LayerNorm.bias as encoder.layer.21.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.21.intermediate.dense.weight as encoder.layer.21.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.21.intermediate.dense.bias as encoder.layer.21.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.21.output.dense.weight as encoder.layer.21.output.dense.weight.\n",
      "SAVING bert.encoder.layer.21.output.dense.bias as encoder.layer.21.output.dense.bias.\n",
      "SAVING bert.encoder.layer.21.output.LayerNorm.weight as encoder.layer.21.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.21.output.LayerNorm.bias as encoder.layer.21.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.22.attention.self.query.weight as encoder.layer.22.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.22.attention.self.query.bias as encoder.layer.22.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.22.attention.self.key.weight as encoder.layer.22.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.22.attention.self.key.bias as encoder.layer.22.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.22.attention.self.value.weight as encoder.layer.22.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.22.attention.self.value.bias as encoder.layer.22.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.22.attention.output.dense.weight as encoder.layer.22.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.22.attention.output.dense.bias as encoder.layer.22.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.22.attention.output.LayerNorm.weight as encoder.layer.22.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.22.attention.output.LayerNorm.bias as encoder.layer.22.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.22.intermediate.dense.weight as encoder.layer.22.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.22.intermediate.dense.bias as encoder.layer.22.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.22.output.dense.weight as encoder.layer.22.output.dense.weight.\n",
      "SAVING bert.encoder.layer.22.output.dense.bias as encoder.layer.22.output.dense.bias.\n",
      "SAVING bert.encoder.layer.22.output.LayerNorm.weight as encoder.layer.22.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.22.output.LayerNorm.bias as encoder.layer.22.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.23.attention.self.query.weight as encoder.layer.23.attention.self.query.weight.\n",
      "SAVING bert.encoder.layer.23.attention.self.query.bias as encoder.layer.23.attention.self.query.bias.\n",
      "SAVING bert.encoder.layer.23.attention.self.key.weight as encoder.layer.23.attention.self.key.weight.\n",
      "SAVING bert.encoder.layer.23.attention.self.key.bias as encoder.layer.23.attention.self.key.bias.\n",
      "SAVING bert.encoder.layer.23.attention.self.value.weight as encoder.layer.23.attention.self.value.weight.\n",
      "SAVING bert.encoder.layer.23.attention.self.value.bias as encoder.layer.23.attention.self.value.bias.\n",
      "SAVING bert.encoder.layer.23.attention.output.dense.weight as encoder.layer.23.attention.output.dense.weight.\n",
      "SAVING bert.encoder.layer.23.attention.output.dense.bias as encoder.layer.23.attention.output.dense.bias.\n",
      "SAVING bert.encoder.layer.23.attention.output.LayerNorm.weight as encoder.layer.23.attention.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.23.attention.output.LayerNorm.bias as encoder.layer.23.attention.output.LayerNorm.bias.\n",
      "SAVING bert.encoder.layer.23.intermediate.dense.weight as encoder.layer.23.intermediate.dense.weight.\n",
      "SAVING bert.encoder.layer.23.intermediate.dense.bias as encoder.layer.23.intermediate.dense.bias.\n",
      "SAVING bert.encoder.layer.23.output.dense.weight as encoder.layer.23.output.dense.weight.\n",
      "SAVING bert.encoder.layer.23.output.dense.bias as encoder.layer.23.output.dense.bias.\n",
      "SAVING bert.encoder.layer.23.output.LayerNorm.weight as encoder.layer.23.output.LayerNorm.weight.\n",
      "SAVING bert.encoder.layer.23.output.LayerNorm.bias as encoder.layer.23.output.LayerNorm.bias.\n",
      "SAVING bert.pooler.dense.weight as pooler.dense.weight.\n",
      "SAVING bert.pooler.dense.bias as pooler.dense.bias.\n",
      "MODIFYING: bert.img_embedding.weight\n",
      "SAVING bert.img_embedding.bias as img_embedding.bias.\n",
      "\n",
      "Weights in loaded but not in model:\n",
      "cls.predictions.bias\n",
      "cls.predictions.decoder.weight\n",
      "cls.predictions.transform.LayerNorm.bias\n",
      "cls.predictions.transform.LayerNorm.weight\n",
      "cls.predictions.transform.dense.bias\n",
      "cls.predictions.transform.dense.weight\n",
      "cls.seq_relationship.bias\n",
      "cls.seq_relationship.weight\n",
      "\n",
      "Weights in model but not in loaded:\n",
      "\n",
      "Total Iters: 2814\n",
      "/opt/conda/lib/python3.7/site-packages/torchcontrib/optim/swa.py:130: UserWarning: Casting swa_start, swa_freq to int\n",
      "  warnings.warn(\"Casting swa_start, swa_freq to int\")\n",
      "Splits in Train data: ['traindev']\n",
      "Splits in Valid data: ['dev_seen']\n",
      "Batches: 938\n",
      "tensor([-1.0056, -0.4554], device='cuda:0')\n",
      "\n",
      "Epoch(U) 0(250): Train AC 60.20 RA 54.7340 LOSS 1371.9015\n",
      "\n",
      "Epoch(U) 0(250): DEV AC 54.40 RA 60.9584 \n",
      "Epoch(U) 0(250): BEST AC 54.40 RA 60.9584 \n",
      "\n",
      "Epoch(U) 0(500): Train AC 63.22 RA 60.8530 LOSS 1259.1461\n",
      "\n",
      "Epoch(U) 0(500): DEV AC 55.80 RA 68.5888 \n",
      "Epoch(U) 0(500): BEST AC 55.80 RA 68.5888 \n",
      "\n",
      "Epoch(U) 0(750): Train AC 64.97 RA 63.8221 LOSS 1255.0365\n",
      "\n",
      "Epoch(U) 0(750): DEV AC 58.00 RA 69.7072 \n",
      "Epoch(U) 0(750): BEST AC 58.00 RA 69.7072 \n",
      "tensor([-0.1580, -1.9231], device='cuda:0')\n",
      "\n",
      "Epoch(U) 1(1000): Train AC 71.98 RA 78.1101 LOSS 1130.6756\n",
      "\n",
      "Epoch(U) 1(1000): DEV AC 63.00 RA 72.2528 \n",
      "Epoch(U) 1(1000): BEST AC 63.00 RA 72.2528 \n",
      "\n",
      "Epoch(U) 1(1250): Train AC 73.88 RA 79.2800 LOSS 1056.6274\n",
      "\n",
      "Epoch(U) 1(1250): DEV AC 66.40 RA 78.2496 \n",
      "Epoch(U) 1(1250): BEST AC 66.40 RA 78.2496 \n",
      "\n",
      "Epoch(U) 1(1500): Train AC 75.11 RA 80.4877 LOSS 999.3541\n",
      "\n",
      "Epoch(U) 1(1500): DEV AC 64.80 RA 79.1536 \n",
      "Epoch(U) 1(1500): BEST AC 64.80 RA 79.1536 \n",
      "\n",
      "Epoch(U) 1(1750): Train AC 75.31 RA 81.1734 LOSS 999.6692\n",
      "\n",
      "Epoch(U) 1(1750): DEV AC 71.80 RA 80.5280 \n",
      "Epoch(U) 1(1750): BEST AC 71.80 RA 80.5280 \n",
      "tensor([-1.2914, -0.3214], device='cuda:0')\n",
      "\n",
      "Epoch(U) 2(2000): Train AC 84.78 RA 90.4651 LOSS 778.9932\n",
      "\n",
      "Epoch(U) 2(2000): DEV AC 73.60 RA 84.3760 \n",
      "Epoch(U) 2(2000): BEST AC 73.60 RA 84.3760 \n",
      "\n",
      "Epoch(U) 2(2250): Train AC 82.62 RA 89.1395 LOSS 862.3286\n",
      "\n",
      "Epoch(U) 2(2250): DEV AC 76.60 RA 84.5520 \n",
      "Epoch(U) 2(2250): BEST AC 76.60 RA 84.5520 \n",
      "\n",
      "Epoch(U) 2(2500): Train AC 81.33 RA 87.1757 LOSS 1011.3247\n",
      "\n",
      "Epoch(U) 2(2500): DEV AC 77.60 RA 86.4544 \n",
      "Epoch(U) 2(2500): BEST AC 77.60 RA 86.4544 \n",
      "\n",
      "Epoch(U) 2(2750): Train AC 80.84 RA 86.8929 LOSS 927.7970\n",
      "\n",
      "Epoch(U) 2(2750): DEV AC 78.20 RA 89.0640 \n",
      "Epoch(U) 2(2750): BEST AC 78.20 RA 89.0640 \n",
      "Load model from ./data/LASTtraindev.pth\n",
      "Load 1000 data from split(s) test_seen.\n",
      "Start to load Faster-RCNN detected objects from data/HM_img.tsv\n",
      "Loaded 1000 images in file data/HM_img.tsv in 35 seconds.\n",
      "Use 1000 data in torch dataset\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   id      1000 non-null   int64  \n",
      " 1   proba   1000 non-null   float64\n",
      " 2   label   1000 non-null   int64  \n",
      "dtypes: float64(1), int64(2)\n",
      "memory usage: 23.6 KB\n",
      "None\n",
      "Load 1000 data from split(s) test_unseen.\n",
      "Start to load Faster-RCNN detected objects from data/HM_img.tsv\n",
      "Loaded 1000 images in file data/HM_img.tsv in 35 seconds.\n",
      "Use 1000 data in torch dataset\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   id      1000 non-null   int64  \n",
      " 1   proba   1000 non-null   float64\n",
      " 2   label   1000 non-null   int64  \n",
      "dtypes: float64(1), int64(2)\n",
      "memory usage: 23.6 KB\n",
      "None\n",
      "Included in Simple Average:  O36_dev_seen.csv\n",
      "Included in Simple Average:  O36_test_seen.csv\n",
      "Included in Simple Average:  O36_test_unseen.csv\n",
      "Included in Simple Average:  U36_dev.csv\n",
      "Included in Simple Average:  U36_dev_seen.csv\n",
      "Included in Simple Average:  U36_test.csv\n",
      "Included in Simple Average:  U36_test_seen.csv\n"
     ]
    }
   ],
   "source": [
    "!cd vilio; bash bash/training/O/hm_O.sh 7500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd4568-77ee-4e61-8983-72f7738dc63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
