{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258c7b4e-ef38-47fe-8ab5-198894a63f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  category=UserWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/omegaconf/resolvers/__init__.py:13: UserWarning: The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\n",
      "  \"The `env` resolver is deprecated, see https://github.com/omry/omegaconf/issues/573\"\n",
      "\u001b[32m2022-05-06T11:12:57 | mmf.utils.configuration: \u001b[0mOverriding option config to finetune_visual_bert.ymal\n",
      "\u001b[32m2022-05-06T11:12:57 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
      "\u001b[32m2022-05-06T11:12:57 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "/opt/conda/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  category=UserWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  category=UserWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  category=UserWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/omegaconf/grammar_visitor.py:257: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.\n",
      "  category=UserWarning,\n",
      "\u001b[32m2022-05-06T11:12:57 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2022-05-06T11:12:57 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=finetune_visual_bert.ymal', 'model=visual_bert', 'dataset=hateful_memes'])\n",
      "\u001b[32m2022-05-06T11:12:57 | mmf_cli.run: \u001b[0mTorch version: 1.9.0+cu102\n",
      "\u001b[32m2022-05-06T11:12:57 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla T4\n",
      "\u001b[32m2022-05-06T11:12:57 | mmf_cli.run: \u001b[0mUsing seed 57129872\n",
      "\u001b[32m2022-05-06T11:12:57 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jupyter/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/jupyter/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/jupyter/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/jupyter/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/jupyter/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "\u001b[32m2022-05-06T11:12:57 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2022-05-06T11:12:57 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2022-05-06T11:12:57 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2022-05-06T11:12:57 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bert_model_name\": \"bert-base-uncased\",\n",
      "  \"bypass_transformer\": false,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_strategy\": \"plain\",\n",
      "  \"finetune_lr_multiplier\": 1,\n",
      "  \"freeze_base\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"losses\": [\n",
      "    \"cross_entropy\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model\": \"visual_bert\",\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_strategy\": \"default\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"random_initialize\": false,\n",
      "  \"special_visual_initialize\": true,\n",
      "  \"training_head_type\": \"classification\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"visual_embedding_dim\": 2048,\n",
      "  \"vocab_size\": 30522,\n",
      "  \"zerobias\": false\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/jupyter/.cache/torch/mmf/distributed_-1/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing VisualBERTBase: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing VisualBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisualBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.bias', 'bert.embeddings.projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2022-05-06T11:13:07 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2022-05-06T11:13:07 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2022-05-06T11:13:07 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-06T11:13:11 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-06T11:13:11 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-06T11:13:11 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-05-06T11:13:11 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
      "  (model): VisualBERTForClassification(\n",
      "    (bert): VisualBERTBase(\n",
      "      (embeddings): BertVisioLinguisticEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (token_type_embeddings_visual): Embedding(2, 768)\n",
      "        (position_embeddings_visual): Embedding(512, 768)\n",
      "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder): BertEncoderJit(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
      "\u001b[32m2022-05-06T11:13:11 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
      "\u001b[32m2022-05-06T11:15:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.7160, train/hateful_memes/cross_entropy/avg: 0.7160, train/total_loss: 0.7160, train/total_loss/avg: 0.7160, max mem: 9175.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 0.74, time: 02m 15s 729ms, time_since_start: 02m 20s 010ms, eta: 08h 23m 20s 326ms\n",
      "\u001b[32m2022-05-06T11:17:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.6268, train/hateful_memes/cross_entropy/avg: 0.6714, train/total_loss: 0.6268, train/total_loss/avg: 0.6714, max mem: 9175.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0.00001, ups: 0.70, time: 02m 22s 396ms, time_since_start: 04m 42s 407ms, eta: 08h 45m 39s 213ms\n",
      "\u001b[32m2022-05-06T11:20:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.6268, train/hateful_memes/cross_entropy/avg: 0.6011, train/total_loss: 0.6268, train/total_loss/avg: 0.6011, max mem: 9175.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0.00001, ups: 0.70, time: 02m 22s 510ms, time_since_start: 07m 04s 918ms, eta: 08h 43m 39s 577ms\n",
      "\u001b[32m2022-05-06T11:22:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.6268, train/hateful_memes/cross_entropy/avg: 0.6232, train/total_loss: 0.6268, train/total_loss/avg: 0.6232, max mem: 9175.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0.00001, ups: 0.71, time: 02m 21s 799ms, time_since_start: 09m 26s 717ms, eta: 08h 38m 38s 647ms\n",
      "\u001b[32m2022-05-06T11:24:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.6563, train/hateful_memes/cross_entropy/avg: 0.6298, train/total_loss: 0.6563, train/total_loss/avg: 0.6298, max mem: 9175.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0.00001, ups: 0.70, time: 02m 22s 293ms, time_since_start: 11m 49s 010ms, eta: 08h 38m 02s 627ms\n",
      "\u001b[32m2022-05-06T11:27:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.6268, train/hateful_memes/cross_entropy/avg: 0.6015, train/total_loss: 0.6268, train/total_loss/avg: 0.6015, max mem: 9175.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0.00002, ups: 0.70, time: 02m 22s 884ms, time_since_start: 14m 11s 895ms, eta: 08h 37m 46s 603ms\n",
      "\u001b[32m2022-05-06T11:29:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.6268, train/hateful_memes/cross_entropy/avg: 0.5803, train/total_loss: 0.6268, train/total_loss/avg: 0.5803, max mem: 9175.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0.00002, ups: 0.71, time: 02m 21s 899ms, time_since_start: 16m 33s 795ms, eta: 08h 31m 48s 207ms\n",
      "\u001b[32m2022-05-06T11:32:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.4606, train/hateful_memes/cross_entropy/avg: 0.5447, train/total_loss: 0.4606, train/total_loss/avg: 0.5447, max mem: 9175.0, experiment: run, epoch: 4, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0.00002, ups: 0.69, time: 02m 24s 239ms, time_since_start: 18m 58s 035ms, eta: 08h 37m 48s 131ms\n",
      "\u001b[32m2022-05-06T11:34:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.4606, train/hateful_memes/cross_entropy/avg: 0.5328, train/total_loss: 0.4606, train/total_loss/avg: 0.5328, max mem: 9175.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0.00002, ups: 0.71, time: 02m 21s 825ms, time_since_start: 21m 19s 861ms, eta: 08h 26m 44s 067ms\n",
      "\u001b[32m2022-05-06T11:36:49 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2022-05-06T11:36:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2022-05-06T11:36:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2022-05-06T11:37:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2022-05-06T11:37:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.4601, train/hateful_memes/cross_entropy/avg: 0.5105, train/total_loss: 0.4601, train/total_loss/avg: 0.5105, max mem: 9175.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00003, ups: 0.66, time: 02m 32s 840ms, time_since_start: 23m 52s 701ms, eta: 09h 03m 30s 118ms\n",
      "\u001b[32m2022-05-06T11:37:00 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2022-05-06T11:37:00 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2022-05-06T11:37:36 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 16\n",
      "\u001b[32m2022-05-06T11:37:36 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2022-05-06T11:37:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2022-05-06T11:37:50 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2022-05-06T11:37:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2022-05-06T11:38:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2022-05-06T11:38:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/cross_entropy: 1.0943, val/total_loss: 1.0943, val/hateful_memes/accuracy: 0.5760, val/hateful_memes/binary_f1: 0.3291, val/hateful_memes/roc_auc: 0.7132, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 01m 08s 031ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.713168\n",
      "\u001b[32m2022-05-06T11:40:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.4601, train/hateful_memes/cross_entropy/avg: 0.4980, train/total_loss: 0.4601, train/total_loss/avg: 0.4980, max mem: 9227.0, experiment: run, epoch: 5, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00003, ups: 0.69, time: 02m 25s 148ms, time_since_start: 27m 25s 883ms, eta: 08h 33m 41s 380ms\n",
      "\u001b[32m2022-05-06T11:42:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.4532, train/hateful_memes/cross_entropy/avg: 0.4921, train/total_loss: 0.4532, train/total_loss/avg: 0.4921, max mem: 9227.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00003, ups: 0.71, time: 02m 21s 684ms, time_since_start: 29m 47s 567ms, eta: 08h 19m 01s 872ms\n",
      "\u001b[32m2022-05-06T11:45:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.4532, train/hateful_memes/cross_entropy/avg: 0.4829, train/total_loss: 0.4532, train/total_loss/avg: 0.4829, max mem: 9227.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00003, ups: 0.70, time: 02m 22s 021ms, time_since_start: 32m 09s 589ms, eta: 08h 17m 48s 847ms\n",
      "\u001b[32m2022-05-06T11:47:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.4379, train/hateful_memes/cross_entropy/avg: 0.4608, train/total_loss: 0.4379, train/total_loss/avg: 0.4608, max mem: 9227.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00003, ups: 0.71, time: 02m 21s 539ms, time_since_start: 34m 31s 128ms, eta: 08h 13m 43s 637ms\n",
      "\u001b[32m2022-05-06T11:50:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.4379, train/hateful_memes/cross_entropy/avg: 0.4450, train/total_loss: 0.4379, train/total_loss/avg: 0.4450, max mem: 9227.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00004, ups: 0.71, time: 02m 21s 605ms, time_since_start: 36m 52s 734ms, eta: 08h 11m 33s 651ms\n",
      "\u001b[32m2022-05-06T11:52:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.4268, train/hateful_memes/cross_entropy/avg: 0.4240, train/total_loss: 0.4268, train/total_loss/avg: 0.4240, max mem: 9227.0, experiment: run, epoch: 7, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00004, ups: 0.70, time: 02m 22s 653ms, time_since_start: 39m 15s 388ms, eta: 08h 12m 47s 036ms\n",
      "\u001b[32m2022-05-06T11:54:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.4268, train/hateful_memes/cross_entropy/avg: 0.4046, train/total_loss: 0.4268, train/total_loss/avg: 0.4046, max mem: 9227.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00004, ups: 0.70, time: 02m 22s 124ms, time_since_start: 41m 37s 512ms, eta: 08h 08m 32s 827ms\n",
      "\u001b[32m2022-05-06T11:57:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.3735, train/hateful_memes/cross_entropy/avg: 0.3904, train/total_loss: 0.3735, train/total_loss/avg: 0.3904, max mem: 9227.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00005, ups: 0.71, time: 02m 21s 941ms, time_since_start: 43m 59s 454ms, eta: 08h 05m 30s 909ms\n",
      "\u001b[32m2022-05-06T11:59:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.3735, train/hateful_memes/cross_entropy/avg: 0.3789, train/total_loss: 0.3735, train/total_loss/avg: 0.3789, max mem: 9227.0, experiment: run, epoch: 8, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00005, ups: 0.69, time: 02m 24s 510ms, time_since_start: 46m 23s 964ms, eta: 08h 11m 51s 320ms\n",
      "\u001b[32m2022-05-06T12:01:53 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2022-05-06T12:01:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2022-05-06T12:01:57 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2022-05-06T12:02:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2022-05-06T12:02:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.3726, train/hateful_memes/cross_entropy/avg: 0.3753, train/total_loss: 0.3726, train/total_loss/avg: 0.3753, max mem: 9227.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00005, ups: 0.62, time: 02m 42s 637ms, time_since_start: 49m 06s 602ms, eta: 09h 10m 48s 007ms\n",
      "\u001b[32m2022-05-06T12:02:14 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2022-05-06T12:02:14 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2022-05-06T12:02:41 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 16\n",
      "\u001b[32m2022-05-06T12:02:41 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2022-05-06T12:02:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2022-05-06T12:02:54 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2022-05-06T12:03:06 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2022-05-06T12:03:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2022-05-06T12:03:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/cross_entropy: 1.5047, val/total_loss: 1.5047, val/hateful_memes/accuracy: 0.5880, val/hateful_memes/binary_f1: 0.3832, val/hateful_memes/roc_auc: 0.7197, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 22000, val_time: 01m 04s 973ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.719712\n",
      "\u001b[32m2022-05-06T12:05:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.3096, train/hateful_memes/cross_entropy/avg: 0.3629, train/total_loss: 0.3096, train/total_loss/avg: 0.3629, max mem: 9227.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00005, ups: 0.70, time: 02m 23s 479ms, time_since_start: 52m 35s 057ms, eta: 08h 03m 29s 209ms\n",
      "\u001b[32m2022-05-06T12:08:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.3081, train/hateful_memes/cross_entropy/avg: 0.3584, train/total_loss: 0.3081, train/total_loss/avg: 0.3584, max mem: 9227.0, experiment: run, epoch: 9, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00005, ups: 0.70, time: 02m 22s 892ms, time_since_start: 54m 57s 949ms, eta: 07h 59m 05s 429ms\n",
      "\u001b[32m2022-05-06T12:10:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.2952, train/hateful_memes/cross_entropy/avg: 0.3535, train/total_loss: 0.2952, train/total_loss/avg: 0.3535, max mem: 9227.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00005, ups: 0.71, time: 02m 21s 883ms, time_since_start: 57m 19s 833ms, eta: 07h 53m 18s 292ms\n",
      "\u001b[32m2022-05-06T12:12:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.2642, train/hateful_memes/cross_entropy/avg: 0.3412, train/total_loss: 0.2642, train/total_loss/avg: 0.3412, max mem: 9227.0, experiment: run, epoch: 10, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00005, ups: 0.70, time: 02m 23s 231ms, time_since_start: 59m 43s 065ms, eta: 07h 55m 22s 545ms\n",
      "\u001b[32m2022-05-06T12:15:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.2509, train/hateful_memes/cross_entropy/avg: 0.3376, train/total_loss: 0.2509, train/total_loss/avg: 0.3376, max mem: 9227.0, experiment: run, epoch: 10, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00005, ups: 0.70, time: 02m 22s 323ms, time_since_start: 01h 02m 05s 388ms, eta: 07h 49m 57s 074ms\n",
      "\u001b[32m2022-05-06T12:17:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.2469, train/hateful_memes/cross_entropy/avg: 0.3317, train/total_loss: 0.2469, train/total_loss/avg: 0.3317, max mem: 9227.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00005, ups: 0.70, time: 02m 22s 411ms, time_since_start: 01h 04m 27s 799ms, eta: 07h 47m 49s 814ms\n",
      "\u001b[32m2022-05-06T12:19:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.2244, train/hateful_memes/cross_entropy/avg: 0.3222, train/total_loss: 0.2244, train/total_loss/avg: 0.3222, max mem: 9227.0, experiment: run, epoch: 11, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00005, ups: 0.70, time: 02m 23s 935ms, time_since_start: 01h 06m 51s 735ms, eta: 07h 50m 24s 035ms\n",
      "\u001b[32m2022-05-06T12:22:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.1837, train/hateful_memes/cross_entropy/avg: 0.3166, train/total_loss: 0.1837, train/total_loss/avg: 0.3166, max mem: 9227.0, experiment: run, epoch: 11, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00005, ups: 0.70, time: 02m 22s 160ms, time_since_start: 01h 09m 13s 896ms, eta: 07h 42m 11s 597ms\n",
      "\u001b[32m2022-05-06T12:24:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.1734, train/hateful_memes/cross_entropy/avg: 0.3070, train/total_loss: 0.1734, train/total_loss/avg: 0.3070, max mem: 9227.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00005, ups: 0.70, time: 02m 22s 404ms, time_since_start: 01h 11m 36s 300ms, eta: 07h 40m 34s 452ms\n",
      "\u001b[32m2022-05-06T12:27:07 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2022-05-06T12:27:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2022-05-06T12:27:11 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2022-05-06T12:27:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2022-05-06T12:27:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.1709, train/hateful_memes/cross_entropy/avg: 0.2969, train/total_loss: 0.1709, train/total_loss/avg: 0.2969, max mem: 9227.0, experiment: run, epoch: 12, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00005, ups: 0.61, time: 02m 44s 598ms, time_since_start: 01h 14m 20s 899ms, eta: 08h 49m 34s 195ms\n",
      "\u001b[32m2022-05-06T12:27:28 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2022-05-06T12:27:28 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2022-05-06T12:27:54 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 16\n",
      "\u001b[32m2022-05-06T12:27:54 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2022-05-06T12:27:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2022-05-06T12:28:08 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2022-05-06T12:28:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2022-05-06T12:28:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 1.6379, val/total_loss: 1.6379, val/hateful_memes/accuracy: 0.6380, val/hateful_memes/binary_f1: 0.6039, val/hateful_memes/roc_auc: 0.7058, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 22000, val_time: 51s 879ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.719712\n",
      "\u001b[32m2022-05-06T12:30:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.1671, train/hateful_memes/cross_entropy/avg: 0.2894, train/total_loss: 0.1671, train/total_loss/avg: 0.2894, max mem: 9227.0, experiment: run, epoch: 12, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00005, ups: 0.70, time: 02m 23s 569ms, time_since_start: 01h 17m 36s 350ms, eta: 07h 39m 28s 752ms\n",
      "\u001b[32m2022-05-06T12:33:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.1495, train/hateful_memes/cross_entropy/avg: 0.2828, train/total_loss: 0.1495, train/total_loss/avg: 0.2828, max mem: 9227.0, experiment: run, epoch: 13, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00005, ups: 0.70, time: 02m 22s 645ms, time_since_start: 01h 19m 58s 996ms, eta: 07h 34m 06s 523ms\n",
      "\u001b[32m2022-05-06T12:35:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.1495, train/hateful_memes/cross_entropy/avg: 0.2806, train/total_loss: 0.1495, train/total_loss/avg: 0.2806, max mem: 9227.0, experiment: run, epoch: 13, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00005, ups: 0.70, time: 02m 22s 597ms, time_since_start: 01h 22m 21s 594ms, eta: 07h 31m 32s 435ms\n",
      "\u001b[32m2022-05-06T12:37:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.1235, train/hateful_memes/cross_entropy/avg: 0.2760, train/total_loss: 0.1235, train/total_loss/avg: 0.2760, max mem: 9227.0, experiment: run, epoch: 13, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00005, ups: 0.70, time: 02m 22s 310ms, time_since_start: 01h 24m 43s 904ms, eta: 07h 28m 13s 291ms\n",
      "\u001b[32m2022-05-06T12:40:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.1130, train/hateful_memes/cross_entropy/avg: 0.2701, train/total_loss: 0.1130, train/total_loss/avg: 0.2701, max mem: 9227.0, experiment: run, epoch: 14, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00005, ups: 0.70, time: 02m 23s 213ms, time_since_start: 01h 27m 07s 117ms, eta: 07h 28m 38s 321ms\n",
      "\u001b[32m2022-05-06T12:42:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.1130, train/hateful_memes/cross_entropy/avg: 0.2629, train/total_loss: 0.1130, train/total_loss/avg: 0.2629, max mem: 9227.0, experiment: run, epoch: 14, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00005, ups: 0.70, time: 02m 22s 486ms, time_since_start: 01h 29m 29s 604ms, eta: 07h 23m 57s 046ms\n",
      "\u001b[32m2022-05-06T12:44:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.1130, train/hateful_memes/cross_entropy/avg: 0.2575, train/total_loss: 0.1130, train/total_loss/avg: 0.2575, max mem: 9227.0, experiment: run, epoch: 14, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00005, ups: 0.70, time: 02m 22s 438ms, time_since_start: 01h 31m 52s 043ms, eta: 07h 21m 23s 298ms\n",
      "\u001b[32m2022-05-06T12:47:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.0931, train/hateful_memes/cross_entropy/avg: 0.2532, train/total_loss: 0.0931, train/total_loss/avg: 0.2532, max mem: 9227.0, experiment: run, epoch: 15, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00005, ups: 0.70, time: 02m 23s 037ms, time_since_start: 01h 34m 15s 080ms, eta: 07h 20m 49s 310ms\n",
      "\u001b[32m2022-05-06T12:49:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.0931, train/hateful_memes/cross_entropy/avg: 0.2532, train/total_loss: 0.0931, train/total_loss/avg: 0.2532, max mem: 9227.0, experiment: run, epoch: 15, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00005, ups: 0.70, time: 02m 22s 252ms, time_since_start: 01h 36m 37s 333ms, eta: 07h 15m 59s 694ms\n",
      "\u001b[32m2022-05-06T12:52:09 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2022-05-06T12:52:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2022-05-06T12:52:12 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2022-05-06T12:52:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2022-05-06T12:52:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.0760, train/hateful_memes/cross_entropy/avg: 0.2478, train/total_loss: 0.0760, train/total_loss/avg: 0.2478, max mem: 9227.0, experiment: run, epoch: 16, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00005, ups: 0.61, time: 02m 44s 125ms, time_since_start: 01h 39m 21s 458ms, eta: 08h 20m 15s 312ms\n",
      "\u001b[32m2022-05-06T12:52:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2022-05-06T12:52:29 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2022-05-06T12:52:57 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 16\n",
      "\u001b[32m2022-05-06T12:52:57 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2022-05-06T12:52:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2022-05-06T12:53:10 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2022-05-06T12:53:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2022-05-06T12:53:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/cross_entropy: 1.8886, val/total_loss: 1.8886, val/hateful_memes/accuracy: 0.6540, val/hateful_memes/binary_f1: 0.5929, val/hateful_memes/roc_auc: 0.6895, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 22000, val_time: 53s 962ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.719712\n",
      "\u001b[32m2022-05-06T12:55:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.0749, train/hateful_memes/cross_entropy/avg: 0.2419, train/total_loss: 0.0749, train/total_loss/avg: 0.2419, max mem: 9227.0, experiment: run, epoch: 16, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00004, ups: 0.70, time: 02m 23s 415ms, time_since_start: 01h 42m 38s 838ms, eta: 07h 14m 42s 105ms\n",
      "\u001b[32m2022-05-06T12:58:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.0693, train/hateful_memes/cross_entropy/avg: 0.2364, train/total_loss: 0.0693, train/total_loss/avg: 0.2364, max mem: 9227.0, experiment: run, epoch: 16, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00004, ups: 0.70, time: 02m 22s 344ms, time_since_start: 01h 45m 01s 182ms, eta: 07h 09m 02s 640ms\n",
      "\u001b[32m2022-05-06T13:00:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.0662, train/hateful_memes/cross_entropy/avg: 0.2323, train/total_loss: 0.0662, train/total_loss/avg: 0.2323, max mem: 9227.0, experiment: run, epoch: 17, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00004, ups: 0.69, time: 02m 24s 002ms, time_since_start: 01h 47m 25s 185ms, eta: 07h 11m 36s 318ms\n",
      "\u001b[32m2022-05-06T13:02:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.0693, train/hateful_memes/cross_entropy/avg: 0.2290, train/total_loss: 0.0693, train/total_loss/avg: 0.2290, max mem: 9227.0, experiment: run, epoch: 17, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00004, ups: 0.70, time: 02m 22s 672ms, time_since_start: 01h 49m 47s 857ms, eta: 07h 05m 12s 094ms\n",
      "\u001b[32m2022-05-06T13:05:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.0693, train/hateful_memes/cross_entropy/avg: 0.2256, train/total_loss: 0.0693, train/total_loss/avg: 0.2256, max mem: 9227.0, experiment: run, epoch: 17, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00004, ups: 0.70, time: 02m 22s 653ms, time_since_start: 01h 52m 10s 511ms, eta: 07h 02m 43s 884ms\n",
      "\u001b[32m2022-05-06T13:07:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.0693, train/hateful_memes/cross_entropy/avg: 0.2223, train/total_loss: 0.0693, train/total_loss/avg: 0.2223, max mem: 9227.0, experiment: run, epoch: 18, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00004, ups: 0.70, time: 02m 23s 453ms, time_since_start: 01h 54m 33s 965ms, eta: 07h 02m 40s 278ms\n",
      "\u001b[32m2022-05-06T13:10:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.0662, train/hateful_memes/cross_entropy/avg: 0.2182, train/total_loss: 0.0662, train/total_loss/avg: 0.2182, max mem: 9227.0, experiment: run, epoch: 18, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00004, ups: 0.70, time: 02m 22s 449ms, time_since_start: 01h 56m 56s 414ms, eta: 06h 57m 18s 059ms\n",
      "\u001b[32m2022-05-06T13:12:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.0615, train/hateful_memes/cross_entropy/avg: 0.2143, train/total_loss: 0.0615, train/total_loss/avg: 0.2143, max mem: 9227.0, experiment: run, epoch: 19, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00004, ups: 0.70, time: 02m 23s 126ms, time_since_start: 01h 59m 19s 541ms, eta: 06h 56m 51s 619ms\n",
      "\u001b[32m2022-05-06T13:14:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.0615, train/hateful_memes/cross_entropy/avg: 0.2102, train/total_loss: 0.0615, train/total_loss/avg: 0.2102, max mem: 9227.0, experiment: run, epoch: 19, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00004, ups: 0.70, time: 02m 22s 328ms, time_since_start: 02h 01m 41s 869ms, eta: 06h 52m 07s 512ms\n",
      "\u001b[32m2022-05-06T13:17:11 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2022-05-06T13:17:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2022-05-06T13:17:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2022-05-06T13:17:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2022-05-06T13:17:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.0662, train/hateful_memes/cross_entropy/avg: 0.2097, train/total_loss: 0.0662, train/total_loss/avg: 0.2097, max mem: 9227.0, experiment: run, epoch: 19, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00004, ups: 0.62, time: 02m 42s 870ms, time_since_start: 02h 04m 24s 739ms, eta: 07h 48m 50s 942ms\n",
      "\u001b[32m2022-05-06T13:17:32 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2022-05-06T13:17:32 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[32m2022-05-06T13:17:59 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished evaluation inference. Loaded 16\n",
      "\u001b[32m2022-05-06T13:17:59 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n",
      "\u001b[32m2022-05-06T13:18:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2022-05-06T13:18:13 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2022-05-06T13:18:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2022-05-06T13:18:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/cross_entropy: 2.0304, val/total_loss: 2.0304, val/hateful_memes/accuracy: 0.6400, val/hateful_memes/binary_f1: 0.5337, val/hateful_memes/roc_auc: 0.6996, num_updates: 5000, epoch: 19, iterations: 5000, max_updates: 22000, val_time: 53s 193ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.719712\n",
      "\u001b[32m2022-05-06T13:20:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.0693, train/hateful_memes/cross_entropy/avg: 0.2105, train/total_loss: 0.0693, train/total_loss/avg: 0.2105, max mem: 9227.0, experiment: run, epoch: 20, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00004, ups: 0.69, time: 02m 25s 609ms, time_since_start: 02h 07m 43s 544ms, eta: 06h 56m 41s 809ms\n",
      "\u001b[32m2022-05-06T13:23:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.0615, train/hateful_memes/cross_entropy/avg: 0.2070, train/total_loss: 0.0615, train/total_loss/avg: 0.2070, max mem: 9227.0, experiment: run, epoch: 20, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00004, ups: 0.70, time: 02m 22s 042ms, time_since_start: 02h 10m 05s 587ms, eta: 06h 44m 04s 986ms\n",
      "\u001b[32m2022-05-06T13:25:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.0602, train/hateful_memes/cross_entropy/avg: 0.2042, train/total_loss: 0.0602, train/total_loss/avg: 0.2042, max mem: 9227.0, experiment: run, epoch: 20, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00004, ups: 0.70, time: 02m 22s 615ms, time_since_start: 02h 12m 28s 202ms, eta: 06h 43m 17s 922ms\n",
      "\u001b[32m2022-05-06T13:27:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.0582, train/hateful_memes/cross_entropy/avg: 0.2014, train/total_loss: 0.0582, train/total_loss/avg: 0.2014, max mem: 9227.0, experiment: run, epoch: 21, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00004, ups: 0.70, time: 02m 23s 065ms, time_since_start: 02h 14m 51s 268ms, eta: 06h 42m 08s 788ms\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=finetune_visual_bert.ymal model=visual_bert dataset=hateful_memes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347211b4-eab1-4a5c-9a1a-75edaf2ed0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "common-cu110.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m91"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
